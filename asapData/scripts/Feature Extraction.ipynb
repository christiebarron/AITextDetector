{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6c6a697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk.tree import Tree\n",
    "import spacy\n",
    "\n",
    "\n",
    "# Initialize spaCy English model\n",
    "nlp_spacy = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc5dd15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract lexical features\n",
    "def extract_lexical_features(text):\n",
    "    # ... your extract_lexical_features function implementation ...\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    total_word_count = len(words)\n",
    "    avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "    avg_sentence_length = sum(len(sentence.split()) for sentence in sentences) / len(sentences)\n",
    "    word_counts = Counter(words)\n",
    "    TTR = len(word_counts) / len(words)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_word_count = sum(1 for word in words if word.lower() in stop_words)\n",
    "    unique_word_count = sum(1 for _, count in word_counts.items() if count == 1)\n",
    "    word_freq = word_counts\n",
    "    bigram_freq = Counter(ngrams(words, 2))\n",
    "    trigram_freq = Counter(ngrams(words, 3))\n",
    "    rare_word_count = sum(1 for _, count in word_counts.items() if count == 1)\n",
    "\n",
    "    return {\n",
    "        'total_word_count': total_word_count,\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'TTR': TTR,\n",
    "        'stop_word_count': stop_word_count,\n",
    "        'unique_word_count': unique_word_count,\n",
    "        'word_freq': word_freq,\n",
    "        'bigram_freq': bigram_freq,\n",
    "        'trigram_freq': trigram_freq,\n",
    "        'rare_word_count': rare_word_count\n",
    "    }\n",
    "\n",
    "#load data from excel file and save as list\n",
    "merged_df = pd.read_excel('../cleanData/processedAsap.xlsx')\n",
    "all_essays = merged_df['essay'].tolist()\n",
    "\n",
    "#extract ai-specific dataset\n",
    "ai_generated_df = merged_df.loc[(merged_df[\"ai_generated\"] == 1), :]\n",
    "ai_generated_essays = ai_generated_df['essay'].tolist()\n",
    "\n",
    "#extract human-specific dataset\n",
    "human_written_df = merged_df.loc[(merged_df[\"ai_generated\"] == 0), :]\n",
    "human_written_essays = human_written_df['essay'].tolist()\n",
    "\n",
    "# Extract lexical features from AI-generated and human-written essays\n",
    "all_lexical_features = [extract_lexical_features(essay) for essay in all_essays]\n",
    "ai_generated_features = [extract_lexical_features(essay) for essay in ai_generated_essays]\n",
    "human_written_features = [extract_lexical_features(essay) for essay in human_written_essays]\n",
    "\n",
    "\n",
    "# Calculate average values of some lexical features for both AI-generated and human-written essays\n",
    "def average_feature_value(features, feature_key):\n",
    "    return sum(feature[feature_key] for feature in features) / len(features)\n",
    "\n",
    "ai_avg_word_length = average_feature_value(ai_generated_features, 'avg_word_length')\n",
    "ai_avg_TTR = average_feature_value(ai_generated_features, 'TTR')\n",
    "ai_avg_stop_word_count = average_feature_value(ai_generated_features, 'stop_word_count')\n",
    "\n",
    "ai_avg_sentence_length = average_feature_value(ai_generated_features, 'avg_sentence_length')\n",
    "human_avg_sentence_length = average_feature_value(human_written_features, 'avg_sentence_length')\n",
    "human_avg_word_length = average_feature_value(human_written_features, 'avg_word_length')\n",
    "human_avg_TTR = average_feature_value(human_written_features, 'TTR')\n",
    "human_avg_stop_word_count = average_feature_value(human_written_features, 'stop_word_count')\n",
    "\n",
    "# Calculate average values of the total word count for both AI-generated and human-written essays\n",
    "ai_avg_total_word_count = average_feature_value(ai_generated_features, 'total_word_count')\n",
    "human_avg_total_word_count = average_feature_value(human_written_features, 'total_word_count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c23d287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_word_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>TTR</th>\n",
       "      <th>stop_word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>word_freq</th>\n",
       "      <th>bigram_freq</th>\n",
       "      <th>trigram_freq</th>\n",
       "      <th>rare_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>386</td>\n",
       "      <td>3.984456</td>\n",
       "      <td>21.125000</td>\n",
       "      <td>0.468912</td>\n",
       "      <td>176</td>\n",
       "      <td>120</td>\n",
       "      <td>{'Dear': 1, 'local': 2, 'newspaper': 1, ',': 1...</td>\n",
       "      <td>{('Dear', 'local'): 1, ('local', 'newspaper'):...</td>\n",
       "      <td>{('Dear', 'local', 'newspaper'): 1, ('local', ...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>464</td>\n",
       "      <td>4.030172</td>\n",
       "      <td>20.950000</td>\n",
       "      <td>0.450431</td>\n",
       "      <td>195</td>\n",
       "      <td>127</td>\n",
       "      <td>{'Dear': 1, '@': 10, 'CAPS1': 1, 'CAPS2': 1, '...</td>\n",
       "      <td>{('Dear', '@'): 1, ('@', 'CAPS1'): 1, ('CAPS1'...</td>\n",
       "      <td>{('Dear', '@', 'CAPS1'): 1, ('@', 'CAPS1', '@'...</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>313</td>\n",
       "      <td>4.035144</td>\n",
       "      <td>19.928571</td>\n",
       "      <td>0.514377</td>\n",
       "      <td>143</td>\n",
       "      <td>111</td>\n",
       "      <td>{'Dear': 1, ',': 9, '@': 7, 'CAPS1': 1, 'CAPS2...</td>\n",
       "      <td>{('Dear', ','): 1, (',', '@'): 2, ('@', 'CAPS1...</td>\n",
       "      <td>{('Dear', ',', '@'): 1, (',', '@', 'CAPS1'): 1...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>611</td>\n",
       "      <td>4.328969</td>\n",
       "      <td>19.407407</td>\n",
       "      <td>0.436989</td>\n",
       "      <td>223</td>\n",
       "      <td>182</td>\n",
       "      <td>{'Dear': 1, 'Local': 1, 'Newspaper': 3, ',': 1...</td>\n",
       "      <td>{('Dear', 'Local'): 1, ('Local', 'Newspaper'):...</td>\n",
       "      <td>{('Dear', 'Local', 'Newspaper'): 1, ('Local', ...</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>517</td>\n",
       "      <td>4.071567</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.408124</td>\n",
       "      <td>241</td>\n",
       "      <td>125</td>\n",
       "      <td>{'Dear': 1, '@': 4, 'LOCATION1': 1, ',': 13, '...</td>\n",
       "      <td>{('Dear', '@'): 1, ('@', 'LOCATION1'): 1, ('LO...</td>\n",
       "      <td>{('Dear', '@', 'LOCATION1'): 1, ('@', 'LOCATIO...</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24412</th>\n",
       "      <td>274</td>\n",
       "      <td>4.664234</td>\n",
       "      <td>20.166667</td>\n",
       "      <td>0.492701</td>\n",
       "      <td>119</td>\n",
       "      <td>90</td>\n",
       "      <td>{'Dear': 1, 'Editor': 1, ',': 16, 'As': 1, 'a'...</td>\n",
       "      <td>{('Dear', 'Editor'): 1, ('Editor', ','): 1, ('...</td>\n",
       "      <td>{('Dear', 'Editor', ','): 1, ('Editor', ',', '...</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24413</th>\n",
       "      <td>247</td>\n",
       "      <td>4.619433</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.603239</td>\n",
       "      <td>101</td>\n",
       "      <td>114</td>\n",
       "      <td>{'Dear': 1, 'Editor': 1, ',': 14, 'As': 1, 'a'...</td>\n",
       "      <td>{('Dear', 'Editor'): 1, ('Editor', ','): 1, ('...</td>\n",
       "      <td>{('Dear', 'Editor', ','): 1, ('Editor', ',', '...</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24414</th>\n",
       "      <td>216</td>\n",
       "      <td>4.717593</td>\n",
       "      <td>15.583333</td>\n",
       "      <td>0.601852</td>\n",
       "      <td>77</td>\n",
       "      <td>100</td>\n",
       "      <td>{'Dear': 1, 'Editor': 1, ',': 12, 'I': 2, 'am'...</td>\n",
       "      <td>{('Dear', 'Editor'): 1, ('Editor', ','): 1, ('...</td>\n",
       "      <td>{('Dear', 'Editor', ','): 1, ('Editor', ',', '...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24415</th>\n",
       "      <td>272</td>\n",
       "      <td>4.977941</td>\n",
       "      <td>19.916667</td>\n",
       "      <td>0.566176</td>\n",
       "      <td>106</td>\n",
       "      <td>110</td>\n",
       "      <td>{'To': 2, 'the': 9, 'Editor': 1, ':': 1, 'As':...</td>\n",
       "      <td>{('To', 'the'): 1, ('the', 'Editor'): 1, ('Edi...</td>\n",
       "      <td>{('To', 'the', 'Editor'): 1, ('the', 'Editor',...</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24416</th>\n",
       "      <td>281</td>\n",
       "      <td>4.569395</td>\n",
       "      <td>15.437500</td>\n",
       "      <td>0.523132</td>\n",
       "      <td>107</td>\n",
       "      <td>104</td>\n",
       "      <td>{'Dear': 1, 'Editor': 1, ',': 19, 'I': 4, 'am'...</td>\n",
       "      <td>{('Dear', 'Editor'): 1, ('Editor', ','): 1, ('...</td>\n",
       "      <td>{('Dear', 'Editor', ','): 1, ('Editor', ',', '...</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24417 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       total_word_count  avg_word_length  avg_sentence_length       TTR  \\\n",
       "0                   386         3.984456            21.125000  0.468912   \n",
       "1                   464         4.030172            20.950000  0.450431   \n",
       "2                   313         4.035144            19.928571  0.514377   \n",
       "3                   611         4.328969            19.407407  0.436989   \n",
       "4                   517         4.071567            15.500000  0.408124   \n",
       "...                 ...              ...                  ...       ...   \n",
       "24412               274         4.664234            20.166667  0.492701   \n",
       "24413               247         4.619433            18.000000  0.603239   \n",
       "24414               216         4.717593            15.583333  0.601852   \n",
       "24415               272         4.977941            19.916667  0.566176   \n",
       "24416               281         4.569395            15.437500  0.523132   \n",
       "\n",
       "       stop_word_count  unique_word_count  \\\n",
       "0                  176                120   \n",
       "1                  195                127   \n",
       "2                  143                111   \n",
       "3                  223                182   \n",
       "4                  241                125   \n",
       "...                ...                ...   \n",
       "24412              119                 90   \n",
       "24413              101                114   \n",
       "24414               77                100   \n",
       "24415              106                110   \n",
       "24416              107                104   \n",
       "\n",
       "                                               word_freq  \\\n",
       "0      {'Dear': 1, 'local': 2, 'newspaper': 1, ',': 1...   \n",
       "1      {'Dear': 1, '@': 10, 'CAPS1': 1, 'CAPS2': 1, '...   \n",
       "2      {'Dear': 1, ',': 9, '@': 7, 'CAPS1': 1, 'CAPS2...   \n",
       "3      {'Dear': 1, 'Local': 1, 'Newspaper': 3, ',': 1...   \n",
       "4      {'Dear': 1, '@': 4, 'LOCATION1': 1, ',': 13, '...   \n",
       "...                                                  ...   \n",
       "24412  {'Dear': 1, 'Editor': 1, ',': 16, 'As': 1, 'a'...   \n",
       "24413  {'Dear': 1, 'Editor': 1, ',': 14, 'As': 1, 'a'...   \n",
       "24414  {'Dear': 1, 'Editor': 1, ',': 12, 'I': 2, 'am'...   \n",
       "24415  {'To': 2, 'the': 9, 'Editor': 1, ':': 1, 'As':...   \n",
       "24416  {'Dear': 1, 'Editor': 1, ',': 19, 'I': 4, 'am'...   \n",
       "\n",
       "                                             bigram_freq  \\\n",
       "0      {('Dear', 'local'): 1, ('local', 'newspaper'):...   \n",
       "1      {('Dear', '@'): 1, ('@', 'CAPS1'): 1, ('CAPS1'...   \n",
       "2      {('Dear', ','): 1, (',', '@'): 2, ('@', 'CAPS1...   \n",
       "3      {('Dear', 'Local'): 1, ('Local', 'Newspaper'):...   \n",
       "4      {('Dear', '@'): 1, ('@', 'LOCATION1'): 1, ('LO...   \n",
       "...                                                  ...   \n",
       "24412  {('Dear', 'Editor'): 1, ('Editor', ','): 1, ('...   \n",
       "24413  {('Dear', 'Editor'): 1, ('Editor', ','): 1, ('...   \n",
       "24414  {('Dear', 'Editor'): 1, ('Editor', ','): 1, ('...   \n",
       "24415  {('To', 'the'): 1, ('the', 'Editor'): 1, ('Edi...   \n",
       "24416  {('Dear', 'Editor'): 1, ('Editor', ','): 1, ('...   \n",
       "\n",
       "                                            trigram_freq  rare_word_count  \n",
       "0      {('Dear', 'local', 'newspaper'): 1, ('local', ...              120  \n",
       "1      {('Dear', '@', 'CAPS1'): 1, ('@', 'CAPS1', '@'...              127  \n",
       "2      {('Dear', ',', '@'): 1, (',', '@', 'CAPS1'): 1...              111  \n",
       "3      {('Dear', 'Local', 'Newspaper'): 1, ('Local', ...              182  \n",
       "4      {('Dear', '@', 'LOCATION1'): 1, ('@', 'LOCATIO...              125  \n",
       "...                                                  ...              ...  \n",
       "24412  {('Dear', 'Editor', ','): 1, ('Editor', ',', '...               90  \n",
       "24413  {('Dear', 'Editor', ','): 1, ('Editor', ',', '...              114  \n",
       "24414  {('Dear', 'Editor', ','): 1, ('Editor', ',', '...              100  \n",
       "24415  {('To', 'the', 'Editor'): 1, ('the', 'Editor',...              110  \n",
       "24416  {('Dear', 'Editor', ','): 1, ('Editor', ',', '...              104  \n",
       "\n",
       "[24417 rows x 10 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6b99076",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to extract syntactic features\n",
    "def extract_syntactic_features(text):\n",
    "    # ... your extract_syntactic_features function implementation ...\n",
    "    doc = nlp_spacy(text)\n",
    "\n",
    "    # Calculate average sentence length\n",
    "    sentence_lengths = [len(sent) for sent in doc.sents]\n",
    "    avg_sentence_length = np.mean(sentence_lengths)\n",
    "\n",
    "    # Calculate parse tree depth\n",
    "    def calc_tree_depth(sent):\n",
    "        root = [token for token in sent if token.head == token][0]\n",
    "        return max([len(list(token.ancestors)) for token in sent])\n",
    "\n",
    "    tree_depths = [calc_tree_depth(sent) for sent in doc.sents]\n",
    "    avg_parse_tree_depth = np.mean(tree_depths)\n",
    "    parse_tree_depth_variation = np.std(tree_depths)\n",
    "\n",
    "    return {\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'avg_parse_tree_depth': avg_parse_tree_depth,\n",
    "        'parse_tree_depth_variation': parse_tree_depth_variation,\n",
    "    }\n",
    "\n",
    "\n",
    "# Extract syntactic features from AI-generated and human-written essays\n",
    "all_syntactic_features = [extract_syntactic_features(essay) for essay in all_essays]\n",
    "ai_generated_syntactic_features = [extract_syntactic_features(essay) for essay in ai_generated_essays]\n",
    "human_written_syntactic_features = [extract_syntactic_features(essay) for essay in human_written_essays]\n",
    "\n",
    "# Compare the syntactic features\n",
    "ai_avg_sentence_length = np.mean([features['avg_sentence_length'] for features in ai_generated_syntactic_features])\n",
    "human_avg_sentence_length = np.mean([features['avg_sentence_length'] for features in human_written_syntactic_features])\n",
    "\n",
    "ai_avg_parse_tree_depth = np.mean([features['avg_parse_tree_depth'] for features in ai_generated_syntactic_features])\n",
    "human_avg_parse_tree_depth = np.mean([features['avg_parse_tree_depth'] for features in human_written_syntactic_features])\n",
    "\n",
    "ai_parse_tree_depth_variation = np.mean([features['parse_tree_depth_variation'] for features in ai_generated_syntactic_features])\n",
    "human_parse_tree_depth_variation = np.mean([features['parse_tree_depth_variation'] for features in human_written_syntactic_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873a75b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_word_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>TTR</th>\n",
       "      <th>stop_word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>word_freq</th>\n",
       "      <th>bigram_freq</th>\n",
       "      <th>trigram_freq</th>\n",
       "      <th>rare_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>242</td>\n",
       "      <td>3.644628</td>\n",
       "      <td>17.333333</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>121</td>\n",
       "      <td>88</td>\n",
       "      <td>{'I': 13, 'remember': 1, 'the': 5, 'first': 1,...</td>\n",
       "      <td>{('I', 'remember'): 1, ('remember', 'the'): 1,...</td>\n",
       "      <td>{('I', 'remember', 'the'): 1, ('remember', 'th...</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>291</td>\n",
       "      <td>4.254296</td>\n",
       "      <td>24.090909</td>\n",
       "      <td>0.426117</td>\n",
       "      <td>130</td>\n",
       "      <td>77</td>\n",
       "      <td>{'The': 8, 'builders': 1, 'of': 11, 'the': 21,...</td>\n",
       "      <td>{('The', 'builders'): 1, ('builders', 'of'): 1...</td>\n",
       "      <td>{('The', 'builders', 'of'): 1, ('builders', 'o...</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>235</td>\n",
       "      <td>3.748936</td>\n",
       "      <td>18.818182</td>\n",
       "      <td>0.527660</td>\n",
       "      <td>118</td>\n",
       "      <td>89</td>\n",
       "      <td>{'It': 1, 'was': 11, 'the': 9, 'night': 2, 'of...</td>\n",
       "      <td>{('It', 'was'): 1, ('was', 'the'): 2, ('the', ...</td>\n",
       "      <td>{('It', 'was', 'the'): 1, ('was', 'the', 'nigh...</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>210</td>\n",
       "      <td>3.671429</td>\n",
       "      <td>16.909091</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>108</td>\n",
       "      <td>88</td>\n",
       "      <td>{'I': 7, 'had': 3, 'been': 1, 'dating': 1, 'my...</td>\n",
       "      <td>{('I', 'had'): 1, ('had', 'been'): 1, ('been',...</td>\n",
       "      <td>{('I', 'had', 'been'): 1, ('had', 'been', 'dat...</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>293</td>\n",
       "      <td>4.354949</td>\n",
       "      <td>19.071429</td>\n",
       "      <td>0.464164</td>\n",
       "      <td>126</td>\n",
       "      <td>93</td>\n",
       "      <td>{'The': 3, 'construction': 1, 'of': 10, 'the':...</td>\n",
       "      <td>{('The', 'construction'): 1, ('construction', ...</td>\n",
       "      <td>{('The', 'construction', 'of'): 1, ('construct...</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2962</th>\n",
       "      <td>274</td>\n",
       "      <td>4.664234</td>\n",
       "      <td>20.166667</td>\n",
       "      <td>0.492701</td>\n",
       "      <td>119</td>\n",
       "      <td>90</td>\n",
       "      <td>{'Dear': 1, 'Editor': 1, ',': 16, 'As': 1, 'a'...</td>\n",
       "      <td>{('Dear', 'Editor'): 1, ('Editor', ','): 1, ('...</td>\n",
       "      <td>{('Dear', 'Editor', ','): 1, ('Editor', ',', '...</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2963</th>\n",
       "      <td>247</td>\n",
       "      <td>4.619433</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.603239</td>\n",
       "      <td>101</td>\n",
       "      <td>114</td>\n",
       "      <td>{'Dear': 1, 'Editor': 1, ',': 14, 'As': 1, 'a'...</td>\n",
       "      <td>{('Dear', 'Editor'): 1, ('Editor', ','): 1, ('...</td>\n",
       "      <td>{('Dear', 'Editor', ','): 1, ('Editor', ',', '...</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2964</th>\n",
       "      <td>216</td>\n",
       "      <td>4.717593</td>\n",
       "      <td>15.583333</td>\n",
       "      <td>0.601852</td>\n",
       "      <td>77</td>\n",
       "      <td>100</td>\n",
       "      <td>{'Dear': 1, 'Editor': 1, ',': 12, 'I': 2, 'am'...</td>\n",
       "      <td>{('Dear', 'Editor'): 1, ('Editor', ','): 1, ('...</td>\n",
       "      <td>{('Dear', 'Editor', ','): 1, ('Editor', ',', '...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2965</th>\n",
       "      <td>272</td>\n",
       "      <td>4.977941</td>\n",
       "      <td>19.916667</td>\n",
       "      <td>0.566176</td>\n",
       "      <td>106</td>\n",
       "      <td>110</td>\n",
       "      <td>{'To': 2, 'the': 9, 'Editor': 1, ':': 1, 'As':...</td>\n",
       "      <td>{('To', 'the'): 1, ('the', 'Editor'): 1, ('Edi...</td>\n",
       "      <td>{('To', 'the', 'Editor'): 1, ('the', 'Editor',...</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2966</th>\n",
       "      <td>281</td>\n",
       "      <td>4.569395</td>\n",
       "      <td>15.437500</td>\n",
       "      <td>0.523132</td>\n",
       "      <td>107</td>\n",
       "      <td>104</td>\n",
       "      <td>{'Dear': 1, 'Editor': 1, ',': 19, 'I': 4, 'am'...</td>\n",
       "      <td>{('Dear', 'Editor'): 1, ('Editor', ','): 1, ('...</td>\n",
       "      <td>{('Dear', 'Editor', ','): 1, ('Editor', ',', '...</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2967 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      total_word_count  avg_word_length  avg_sentence_length       TTR  \\\n",
       "0                  242         3.644628            17.333333  0.545455   \n",
       "1                  291         4.254296            24.090909  0.426117   \n",
       "2                  235         3.748936            18.818182  0.527660   \n",
       "3                  210         3.671429            16.909091  0.566667   \n",
       "4                  293         4.354949            19.071429  0.464164   \n",
       "...                ...              ...                  ...       ...   \n",
       "2962               274         4.664234            20.166667  0.492701   \n",
       "2963               247         4.619433            18.000000  0.603239   \n",
       "2964               216         4.717593            15.583333  0.601852   \n",
       "2965               272         4.977941            19.916667  0.566176   \n",
       "2966               281         4.569395            15.437500  0.523132   \n",
       "\n",
       "      stop_word_count  unique_word_count  \\\n",
       "0                 121                 88   \n",
       "1                 130                 77   \n",
       "2                 118                 89   \n",
       "3                 108                 88   \n",
       "4                 126                 93   \n",
       "...               ...                ...   \n",
       "2962              119                 90   \n",
       "2963              101                114   \n",
       "2964               77                100   \n",
       "2965              106                110   \n",
       "2966              107                104   \n",
       "\n",
       "                                              word_freq  \\\n",
       "0     {'I': 13, 'remember': 1, 'the': 5, 'first': 1,...   \n",
       "1     {'The': 8, 'builders': 1, 'of': 11, 'the': 21,...   \n",
       "2     {'It': 1, 'was': 11, 'the': 9, 'night': 2, 'of...   \n",
       "3     {'I': 7, 'had': 3, 'been': 1, 'dating': 1, 'my...   \n",
       "4     {'The': 3, 'construction': 1, 'of': 10, 'the':...   \n",
       "...                                                 ...   \n",
       "2962  {'Dear': 1, 'Editor': 1, ',': 16, 'As': 1, 'a'...   \n",
       "2963  {'Dear': 1, 'Editor': 1, ',': 14, 'As': 1, 'a'...   \n",
       "2964  {'Dear': 1, 'Editor': 1, ',': 12, 'I': 2, 'am'...   \n",
       "2965  {'To': 2, 'the': 9, 'Editor': 1, ':': 1, 'As':...   \n",
       "2966  {'Dear': 1, 'Editor': 1, ',': 19, 'I': 4, 'am'...   \n",
       "\n",
       "                                            bigram_freq  \\\n",
       "0     {('I', 'remember'): 1, ('remember', 'the'): 1,...   \n",
       "1     {('The', 'builders'): 1, ('builders', 'of'): 1...   \n",
       "2     {('It', 'was'): 1, ('was', 'the'): 2, ('the', ...   \n",
       "3     {('I', 'had'): 1, ('had', 'been'): 1, ('been',...   \n",
       "4     {('The', 'construction'): 1, ('construction', ...   \n",
       "...                                                 ...   \n",
       "2962  {('Dear', 'Editor'): 1, ('Editor', ','): 1, ('...   \n",
       "2963  {('Dear', 'Editor'): 1, ('Editor', ','): 1, ('...   \n",
       "2964  {('Dear', 'Editor'): 1, ('Editor', ','): 1, ('...   \n",
       "2965  {('To', 'the'): 1, ('the', 'Editor'): 1, ('Edi...   \n",
       "2966  {('Dear', 'Editor'): 1, ('Editor', ','): 1, ('...   \n",
       "\n",
       "                                           trigram_freq  rare_word_count  \n",
       "0     {('I', 'remember', 'the'): 1, ('remember', 'th...               88  \n",
       "1     {('The', 'builders', 'of'): 1, ('builders', 'o...               77  \n",
       "2     {('It', 'was', 'the'): 1, ('was', 'the', 'nigh...               89  \n",
       "3     {('I', 'had', 'been'): 1, ('had', 'been', 'dat...               88  \n",
       "4     {('The', 'construction', 'of'): 1, ('construct...               93  \n",
       "...                                                 ...              ...  \n",
       "2962  {('Dear', 'Editor', ','): 1, ('Editor', ',', '...               90  \n",
       "2963  {('Dear', 'Editor', ','): 1, ('Editor', ',', '...              114  \n",
       "2964  {('Dear', 'Editor', ','): 1, ('Editor', ',', '...              100  \n",
       "2965  {('To', 'the', 'Editor'): 1, ('the', 'Editor',...              110  \n",
       "2966  {('Dear', 'Editor', ','): 1, ('Editor', ',', '...              104  \n",
       "\n",
       "[2967 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(all_syntactic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4793bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine lexical and syntactic features\n",
    "def combined_features(text):\n",
    "    lexical = extract_lexical_features(text)\n",
    "    syntactic = extract_syntactic_features(text)\n",
    "    return {**lexical, **syntactic}\n",
    "\n",
    "# Extract combined features for AI-generated and human-written essays\n",
    "all_combined_features = [combined_features(essay) for essay in all_essays]\n",
    "ai_generated_combined_features = [combined_features(essay) for essay in ai_generated_essays]\n",
    "human_written_combined_features = [combined_features(essay) for essay in human_written_essays]\n",
    "\n",
    "# Create a DataFrame for AI-generated essays\n",
    "ai_generated_df = pd.DataFrame(ai_generated_combined_features)\n",
    "ai_generated_df['type'] = 'AI-generated'\n",
    "\n",
    "# Create a DataFrame for human-written essays\n",
    "human_written_df = pd.DataFrame(human_written_combined_features)\n",
    "human_written_df['type'] = 'Human-written'\n",
    "\n",
    "# Calculate the average values of features for both AI-generated and human-written essays\n",
    "def average_feature_value(features, feature_key):\n",
    "    return sum(feature[feature_key] for feature in features) / len(features)\n",
    "\n",
    "# Define a list of feature keys to extract from the combined features\n",
    "feature_keys = [\n",
    "    'total_word_count',\n",
    "    'avg_word_length',\n",
    "    'avg_sentence_length',\n",
    "    'TTR',\n",
    "    'stop_word_count',\n",
    "    'unique_word_count',\n",
    "    'rare_word_count',\n",
    "    'avg_parse_tree_depth',\n",
    "    'parse_tree_depth_variation'\n",
    "]\n",
    "\n",
    "# Calculate the average values for each feature\n",
    "ai_generated_avgs = [average_feature_value(ai_generated_combined_features, key) for key in feature_keys]\n",
    "human_written_avgs = [average_feature_value(human_written_combined_features, key) for key in feature_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc96d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a DataFrame for Combined human and ai essays\n",
    "all_df = pd.concat(merged_df, all_combined_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5864d072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stylistic Features\n",
    "def extract_stylistic_features(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    num_sentences = len(sentences)\n",
    "    \n",
    "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "    pos_tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "    \n",
    "    num_adjectives = sum(sum(1 for word, pos in sentence if pos.startswith('JJ')) for sentence in pos_tagged_sentences)\n",
    "    num_adverbs = sum(sum(1 for word, pos in sentence if pos.startswith('RB')) for sentence in pos_tagged_sentences)\n",
    "    num_verbs = sum(sum(1 for word, pos in sentence if pos.startswith('VB')) for sentence in pos_tagged_sentences)\n",
    "    num_nouns = sum(sum(1 for word, pos in sentence if pos.startswith('NN')) for sentence in pos_tagged_sentences)\n",
    "\n",
    "    avg_adjectives_per_sentence = num_adjectives / num_sentences\n",
    "    avg_adverbs_per_sentence = num_adverbs / num_sentences\n",
    "    avg_verbs_per_sentence = num_verbs / num_sentences\n",
    "    avg_nouns_per_sentence = num_nouns / num_sentences\n",
    "    \n",
    "    return {\n",
    "        'avg_adjectives_per_sentence': avg_adjectives_per_sentence,\n",
    "        'avg_adverbs_per_sentence': avg_adverbs_per_sentence,\n",
    "        'avg_verbs_per_sentence': avg_verbs_per_sentence,\n",
    "        'avg_nouns_per_sentence': avg_nouns_per_sentence,\n",
    "    }\n",
    "\n",
    "# Extract stylistic features from AI-generated and human-written essays\n",
    "all_stylistic_features = [extract_stylistic_features(essay) for essay in all_essays]\n",
    "ai_generated_stylistic_features = [extract_stylistic_features(essay) for essay in ai_generated_essays]\n",
    "human_written_stylistic_features = [extract_stylistic_features(essay) for essay in human_written_essays]\n",
    "\n",
    "# Calculate average values of stylistic features for both AI-generated and human-written essays\n",
    "ai_avg_adjectives_per_sentence = average_feature_value(ai_generated_stylistic_features, 'avg_adjectives_per_sentence')\n",
    "ai_avg_adverbs_per_sentence = average_feature_value(ai_generated_stylistic_features, 'avg_adverbs_per_sentence')\n",
    "ai_avg_verbs_per_sentence = average_feature_value(ai_generated_stylistic_features, 'avg_verbs_per_sentence')\n",
    "ai_avg_nouns_per_sentence = average_feature_value(ai_generated_stylistic_features, 'avg_nouns_per_sentence')\n",
    "\n",
    "human_avg_adjectives_per_sentence = average_feature_value(human_written_stylistic_features, 'avg_adjectives_per_sentence')\n",
    "human_avg_adverbs_per_sentence = average_feature_value(human_written_stylistic_features, 'avg_adverbs_per_sentence')\n",
    "human_avg_verbs_per_sentence = average_feature_value(human_written_stylistic_features, 'avg_verbs_per_sentence')\n",
    "human_avg_nouns_per_sentence = average_feature_value(human_written_stylistic_features, 'avg_nouns_per_sentence')\n",
    "\n",
    "import string\n",
    "\n",
    "def count_punctuation(text):\n",
    "    punctuation_count = sum(1 for char in text if char in string.punctuation)\n",
    "    return punctuation_count\n",
    "\n",
    "def average_value(values):\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "all_avg_punctuation = average_value([count_punctuation(essay) for essay in all_essays])\n",
    "ai_avg_punctuation = average_value([count_punctuation(essay) for essay in ai_generated_essays])\n",
    "human_avg_punctuation = average_value([count_punctuation(essay) for essay in human_written_essays])\n",
    "\n",
    "\n",
    "# ai_avg_punctuation = average_feature_value([(count_punctuation(essay)) for essay in ai_generated_essays])\n",
    "# human_avg_punctuation = average_feature_value([(count_punctuation(essay)) for essay in human_written_essays])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a2bae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the extracted features with the original data\n",
    "pd.concat(merged_df, all_combined_features, all_stylistic_features)\n",
    "\n",
    "#save as excel document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3dc250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = {\n",
    "    'Feature': ['Total Word Count', \n",
    "                'Average Word Length', \n",
    "                'Average Sentence Length', \n",
    "                'Type-Token Ratio', \n",
    "                'Stop Word Count', \n",
    "                'Average Parse Tree Depth', \n",
    "                'Parse Tree Depth Variation', \n",
    "                'Average Adjectives per Sentence', \n",
    "                'Average Adverbs per Sentence', \n",
    "                'Average Verbs per Sentence', \n",
    "                'Average Nouns per Sentence', \n",
    "                'Average Punctuation Marks'],\n",
    "    'AI-Generated': [ai_avg_total_word_count, \n",
    "                     ai_avg_word_length, ai_avg_sentence_length, \n",
    "                     ai_avg_TTR, ai_avg_stop_word_count, \n",
    "                     ai_avg_parse_tree_depth, \n",
    "                     ai_parse_tree_depth_variation, \n",
    "                     ai_avg_adjectives_per_sentence, \n",
    "                     ai_avg_adverbs_per_sentence, \n",
    "                     ai_avg_verbs_per_sentence, \n",
    "                     ai_avg_nouns_per_sentence, \n",
    "                     ai_avg_punctuation],\n",
    "    'Human-Written': [human_avg_total_word_count, \n",
    "                      human_avg_word_length, \n",
    "                      human_avg_sentence_length, \n",
    "                      human_avg_TTR, \n",
    "                      human_avg_stop_word_count, \n",
    "                      human_avg_parse_tree_depth, \n",
    "                      human_parse_tree_depth_variation, \n",
    "                      human_avg_adjectives_per_sentence, \n",
    "                      human_avg_adverbs_per_sentence, \n",
    "                      human_avg_verbs_per_sentence, \n",
    "                      human_avg_nouns_per_sentence, \n",
    "                      human_avg_punctuation],\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# # Save the updated comparison DataFrame to an Excel file\n",
    "# comparison_df.to_excel('feature_comparison.xlsx', index=False)\n",
    "import openpyxl\n",
    "# Save the comparison DataFrame to an Excel file\n",
    "file_name = 'feature_comparison.xlsx'\n",
    "comparison_df.to_excel(file_name, index=False)\n",
    "\n",
    "# Autofit the column widths using openpyxl\n",
    "workbook = openpyxl.load_workbook(file_name)\n",
    "worksheet = workbook.active\n",
    "\n",
    "for column_cells in worksheet.columns:\n",
    "    length = max(len(str(cell.value)) for cell in column_cells)\n",
    "    worksheet.column_dimensions[column_cells[0].column_letter].width = length\n",
    "\n",
    "workbook.save(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4ce3b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>AI-Generated</th>\n",
       "      <th>Human-Written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Word Count</td>\n",
       "      <td>247.460000</td>\n",
       "      <td>384.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Average Word Length</td>\n",
       "      <td>4.194879</td>\n",
       "      <td>3.994626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Average Sentence Length</td>\n",
       "      <td>21.575748</td>\n",
       "      <td>18.759387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Type-Token Ratio</td>\n",
       "      <td>0.529432</td>\n",
       "      <td>0.472312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stop Word Count</td>\n",
       "      <td>111.480000</td>\n",
       "      <td>167.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Avg Parse Tree Depth</td>\n",
       "      <td>5.554981</td>\n",
       "      <td>4.802128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Parse Tree Depth Variation</td>\n",
       "      <td>1.851215</td>\n",
       "      <td>1.676963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Punctuation Count</td>\n",
       "      <td>24.340000</td>\n",
       "      <td>47.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Passive Sentences</td>\n",
       "      <td>1.680000</td>\n",
       "      <td>1.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Flesch Reading Ease</td>\n",
       "      <td>64.923400</td>\n",
       "      <td>73.179600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SMOG Index</td>\n",
       "      <td>10.060000</td>\n",
       "      <td>7.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sentiment Polarity</td>\n",
       "      <td>0.109202</td>\n",
       "      <td>0.187748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sentiment Subjectivity</td>\n",
       "      <td>0.521389</td>\n",
       "      <td>0.465000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Feature  AI-Generated  Human-Written\n",
       "0             Total Word Count    247.460000     384.060000\n",
       "1          Average Word Length      4.194879       3.994626\n",
       "2      Average Sentence Length     21.575748      18.759387\n",
       "3             Type-Token Ratio      0.529432       0.472312\n",
       "4              Stop Word Count    111.480000     167.880000\n",
       "5         Avg Parse Tree Depth      5.554981       4.802128\n",
       "6   Parse Tree Depth Variation      1.851215       1.676963\n",
       "7            Punctuation Count     24.340000      47.760000\n",
       "8            Passive Sentences      1.680000       1.080000\n",
       "9          Flesch Reading Ease     64.923400      73.179600\n",
       "10                  SMOG Index     10.060000       7.900000\n",
       "11          Sentiment Polarity      0.109202       0.187748\n",
       "12      Sentiment Subjectivity      0.521389       0.465000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Initialize spaCy English model\n",
    "nlp_spacy = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to count passive sentences\n",
    "def count_passive_sentences(text):\n",
    "    passive_sentences = 0\n",
    "    doc = nlp_spacy(text)\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'nsubjpass':\n",
    "            passive_sentences += 1\n",
    "    return passive_sentences\n",
    "\n",
    "# Function to calculate readability scores\n",
    "from readability import Readability\n",
    "from readability.exceptions import ReadabilityException\n",
    "\n",
    "\n",
    "import textstat\n",
    "\n",
    "# Function to calculate readability scores\n",
    "def readability_scores(text):\n",
    "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
    "    flesch_kincaid_grade_level = textstat.text_standard(text, float_output=True)\n",
    "    smog_index = textstat.smog_index(text)\n",
    "    return flesch_reading_ease, flesch_kincaid_grade_level, smog_index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to calculate sentiment analysis scores\n",
    "def sentiment_analysis_scores(text):\n",
    "    sentiment = TextBlob(text)\n",
    "    return sentiment.polarity, sentiment.subjectivity\n",
    "\n",
    "# Calculate new features for AI-generated and human-written essays\n",
    "ai_generated_passive_sentences = [count_passive_sentences(essay) for essay in ai_generated_essays]\n",
    "human_written_passive_sentences = [count_passive_sentences(essay) for essay in human_written_essays]\n",
    "\n",
    "ai_generated_readability_scores = [readability_scores(essay) for essay in ai_generated_essays]\n",
    "human_written_readability_scores = [readability_scores(essay) for essay in human_written_essays]\n",
    "\n",
    "ai_generated_sentiment_scores = [sentiment_analysis_scores(essay) for essay in ai_generated_essays]\n",
    "human_written_sentiment_scores = [sentiment_analysis_scores(essay) for essay in human_written_essays]\n",
    "\n",
    "# Calculate average values for the new features\n",
    "ai_avg_passive_sentences = np.mean(ai_generated_passive_sentences)\n",
    "human_avg_passive_sentences = np.mean(human_written_passive_sentences)\n",
    "\n",
    "ai_avg_flesch_reading_ease = np.mean([score[0] for score in ai_generated_readability_scores])\n",
    "human_avg_flesch_reading_ease = np.mean([score[0] for score in human_written_readability_scores])\n",
    "\n",
    "ai_avg_smog_index = np.mean([score[1] for score in ai_generated_readability_scores])\n",
    "human_avg_smog_index = np.mean([score[1] for score in human_written_readability_scores])\n",
    "\n",
    "ai_avg_polarity = np.mean([score[0] for score in ai_generated_sentiment_scores])\n",
    "human_avg_polarity = np.mean([score[0] for score in human_written_sentiment_scores])\n",
    "\n",
    "ai_avg_subjectivity = np.mean([score[1] for score in ai_generated_sentiment_scores])\n",
    "human_avg_subjectivity = np.mean([score[1] for score in human_written_sentiment_scores])\n",
    "\n",
    "# Update comparison_data with the new features\n",
    "comparison_data = {\n",
    "    'Feature': ['Total Word Count', 'Average Word Length', 'Average Sentence Length', 'Type-Token Ratio', 'Stop Word Count', 'Avg Parse Tree Depth', 'Parse Tree Depth Variation', 'Punctuation Count', 'Passive Sentences', 'Flesch Reading Ease', 'SMOG Index', 'Sentiment Polarity', 'Sentiment Subjectivity'],\n",
    "    'AI-Generated': [ai_avg_total_word_count, ai_avg_word_length, ai_avg_sentence_length, ai_avg_TTR, ai_avg_stop_word_count, ai_avg_parse_tree_depth, ai_parse_tree_depth_variation, ai_avg_punctuation, ai_avg_passive_sentences, ai_avg_flesch_reading_ease, ai_avg_smog_index, ai_avg_polarity, ai_avg_subjectivity],\n",
    "    'Human-Written': [human_avg_total_word_count, human_avg_word_length, human_avg_sentence_length, human_avg_TTR, human_avg_stop_word_count, human_avg_parse_tree_depth, human_parse_tree_depth_variation, human_avg_punctuation, human_avg_passive_sentences, human_avg_flesch_reading_ease, human_avg_smog_index, human_avg_polarity, human_avg_subjectivity],\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725723a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
