{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Text Data\n",
    "\n",
    "This script conducts a variety of text pre-processing strategies. They include: cleaning text, tokenizing text, removing special characters, case conversion, correcting spellings, removing stopwords and other unnecessary terms, and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#install the “popular” subset of NLTK data, on the command line type\n",
    "#python -m nltk.downloader popular\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the data\n",
    "asap_df = pd.read_excel(\"../cleanData/mergedAsap.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning all anonymized words (all words that start with @)\n",
    "\n",
    "### TO DO STILL ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning up the raw text: remove punctuation, tokenize, stopword removal\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation]) #remove punctuation\n",
    "    tokens = re.split('\\W+', text) #tokenize\n",
    "    text = [word for word in tokens if word not in stopwords] #remove stopwords\n",
    "\n",
    "asap_df['clean_text_nostop'] = asap_df[\"column_name_of_text\"].apply(lamda x: clean_text(x.lower())) #run the new function through every row of text\n",
    "def stem_text(cleaned_text_to_stem):\n",
    "    text = [ps.stem(word) for word in cleaned_text_to_stem]\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Removing Special Charactes \n",
    "\n",
    "Remove_Special_CharactersDf=asap_df[\"essay\"].str.replace('\\W', ' ', regex=True)\n",
    "Remove_Special_CharactersDf\n",
    "sentence= Remove_Special_CharactersDf.loc[asap_df.index[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear CAPS1 CAPS2 I believe that use computer will benefit us in many way like talk and become friend will others through website like facebook and mysace Using computer can help us find coordibates location and able ourselfs to million of information Also computer will benefit us by help with job as in plan a house plan and type a NUM1 page report for one of our job in less than write it Now let go into the wonder world of technology Using a computer will help us in life by talk or make friend on line Many people have myspace facebooks aim these all benefit us by have conversation with one another Many people believe computer be bad but how can you make friend if you can never talk to them I be very fortunate for have a computer that can help with not only school work but my social life and how I make friend Computers help us with find our location coordibates and million of information online If we didn t go on the internet a lot we wouldn t know how to go onto website that MONTH1 help us with location and coordinate like LOCATION1 Would you rather use a computer or be in LOCATION3 When your suppose to be vacation in LOCATION2 Million of information be find on the internet You can as almost every question and a computer will have it Would you rather easily draw up a house plan on the computer or take NUM1 hour do one by hand with ugly erazer mark all over it you be garrenteed that to find a job with a draw like that Also when appling for a job many worker must write very long paper like a NUM3 word essay on why this job fit you the most and many people I know don t like write NUM3 word non stopp for hour when it could take them I hav an a computer That be why computer we need a lot now adays I hope this essay have impact your descion on computer because they be great machine to work with The other day I show my mom how to use a computer and she say it be the great invention sense slice bread Now go out and buy a computer to help you chat online with friend find location and million of information on one click of the button and help your self with get a job with neat prepare printed work that your bos will love\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/gayanmeerigama/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# lemmatization is a lot more powerful. It looks beyond word reduction and considers a language’s full vocabulary to\n",
    "# apply a morphological analysis to words, aiming to remove inflectional endings only and to return the base or \n",
    "# dictionary form of a word\n",
    "# Wordnet is a publicly available lexical database of over 200 languages that provides semantic relationships betweenits words\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "  \n",
    "sentence= Remove_Special_CharactersDf.loc[asap_df.index[1]]\n",
    " \n",
    "# tokenize the sentence and find the POS tag for each token\n",
    "pos_tagged = nltk.pos_tag(nltk.word_tokenize(sentence)) \n",
    "  \n",
    "# we use our own pos_tagger function to make things simpler to understand.\n",
    "wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "\n",
    " \n",
    "lemmatized_sentence = []\n",
    "for word, tag in wordnet_tagged:\n",
    "    if tag is None:\n",
    "        # if there is no available tag, append the token as is\n",
    "        lemmatized_sentence.append(word)\n",
    "    else:       \n",
    "        # else use the tag to lemmatize the token\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
    " \n",
    "print(lemmatized_sentence)\n"
    
    #The words which are generally filtered out before processing a natural language are called stop words
#Examples of a few stop words in English are “the”, “a”, “an”, “so”, “what”.
#NLTK is a library to play with natural language.The steps to import the library and the English stop words list

from nltk.corpus import stopwords
sw_nltk = stopwords.words('english')

words = [word for word in sentence.split() if word.lower() not in sw_nltk]
new_text = " ".join(words)
print(new_text)
print("Old length: ", len(sentence))
print("New length: ", len(new_text))
    
    
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
