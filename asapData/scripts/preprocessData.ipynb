{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Text Data\n",
    "\n",
    "This script conducts a variety of text pre-processing strategies. They include: cleaning text, tokenizing text, removing special characters, case conversion, correcting spellings, removing stopwords and other unnecessary terms, and lemmatization\n",
    "\n",
    "## To do\n",
    "- manage the human data method of anonymizing with @.\n",
    "- Sentence extraction should occur before removal of punctuation.\n",
    "- trouble-shoot lemmatization df.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/cbarron/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "#install the “popular” subset of NLTK data, on the command line type\n",
    "#python -m nltk.downloader popular\n",
    "\n",
    "import nltk\n",
    "wn = nltk.WordNetLemmatizer() #specifying wn as the word net lemmatizer\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'essay_id', 'essay_set', 'essay', 'ai_generated'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in the data\n",
    "asap_df = pd.read_excel(\"../cleanData/mergedAsap.xlsx\")\n",
    "\n",
    "test_row = asap_df.loc[2,]\n",
    "asap_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning all anonymized words (all words that start with @)\n",
    "\n",
    "### TO DO STILL ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning up the raw text: remove punctuation, tokenize, stopword removal\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def clean_text(text, tokenize_sentence):\n",
    "    sentences = nltk.sent_tokenize(text) #create sentence tokens (not cleaned)\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation]) #remove punctuation\n",
    "    tokens = nltk.tokenize.word_tokenize(text) #tokenize\n",
    "    text = [word for word in tokens if word not in stopwords] #remove stopwords\n",
    "\n",
    "    if (tokenize_sentence == \"y\"):\n",
    "        return(sentences)\n",
    "    else:\n",
    "        return(text)\n",
    "\n",
    "\n",
    "asap_df['word_tokens'] = asap_df[\"essay\"].apply(lambda x: clean_text(x.lower(), tokenize_sentence= \"n\")) #run the new function through every row of text\n",
    "\n",
    "asap_df[\"sentence_tokens\"] = asap_df[\"essay\"].apply(lambda x: clean_text(x.lower(), tokenize_sentence= \"y\"))\n",
    "\n",
    "#def stem_text(cleaned_text_to_stem):\n",
    "#    text = [ps.stem(word) for word in cleaned_text_to_stem]\n",
    "#    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [dear, local, newspaper, think, effects, compu...\n",
       "1        [dear, caps1, caps2, believe, using, computers...\n",
       "2        [dear, caps1, caps2, caps3, people, use, compu...\n",
       "3        [dear, local, newspaper, caps1, found, many, e...\n",
       "4        [dear, location1, know, computers, positive, e...\n",
       "                               ...                        \n",
       "21445    [one, caps1, caps2, bring, people, together, o...\n",
       "21446    [caps8, first, time, going, band, camp, caps1,...\n",
       "21447    [laughter, one, important, things, life, opini...\n",
       "21448    [caps1, caps2, laughter, one, language, speak,...\n",
       "21449    [good, caps1, location1, every, year, select, ...\n",
       "Name: word_tokens, Length: 21450, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asap_df[\"word_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(word_tokens):\n",
    "    lem_text = [wn.lemmatize(word) for word in word_tokens]\n",
    "    return(lem_text)\n",
    "\n",
    "#lemmatize_text(test_row)\n",
    "asap_df[\"lemmatized_word_tokens\"] = asap_df[\"word_tokens\"].apply(lambda x: lemmatize_text(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [dear, local, newspaper, think, effects, compu...\n",
      "1    [dear, caps1, caps2, believe, using, computers...\n",
      "2    [dear, caps1, caps2, caps3, people, use, compu...\n",
      "3    [dear, local, newspaper, caps1, found, many, e...\n",
      "4    [dear, location1, know, computers, positive, e...\n",
      "Name: word_tokens, dtype: object\n",
      "0    [dear local newspaper, i think effects compute...\n",
      "1    [dear @caps1 @caps2, i believe that using comp...\n",
      "2    [dear, @caps1 @caps2 @caps3 more and more peop...\n",
      "3    [dear local newspaper, @caps1 i have found tha...\n",
      "4    [dear @location1, i know having computers has ...\n",
      "Name: sentence_tokens, dtype: object\n",
      "0    [dear, local, newspaper, think, effect, comput...\n",
      "1    [dear, caps1, caps2, believe, using, computer,...\n",
      "2    [dear, caps1, caps2, caps3, people, use, compu...\n",
      "3    [dear, local, newspaper, caps1, found, many, e...\n",
      "4    [dear, location1, know, computer, positive, ef...\n",
      "Name: lemmatized_word_tokens, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>ai_generated</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>lemmatized_word_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>0</td>\n",
       "      <td>[dear, local, newspaper, think, effects, compu...</td>\n",
       "      <td>[dear local newspaper, i think effects compute...</td>\n",
       "      <td>[dear, local, newspaper, think, effect, comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>0</td>\n",
       "      <td>[dear, caps1, caps2, believe, using, computers...</td>\n",
       "      <td>[dear @caps1 @caps2, i believe that using comp...</td>\n",
       "      <td>[dear, caps1, caps2, believe, using, computer,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>0</td>\n",
       "      <td>[dear, caps1, caps2, caps3, people, use, compu...</td>\n",
       "      <td>[dear, @caps1 @caps2 @caps3 more and more peop...</td>\n",
       "      <td>[dear, caps1, caps2, caps3, people, use, compu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>0</td>\n",
       "      <td>[dear, local, newspaper, caps1, found, many, e...</td>\n",
       "      <td>[dear local newspaper, @caps1 i have found tha...</td>\n",
       "      <td>[dear, local, newspaper, caps1, found, many, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[dear, location1, know, computers, positive, e...</td>\n",
       "      <td>[dear @location1, i know having computers has ...</td>\n",
       "      <td>[dear, location1, know, computer, positive, ef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21445</th>\n",
       "      <td>4249</td>\n",
       "      <td>22238</td>\n",
       "      <td>8</td>\n",
       "      <td>Just One @CAPS1 @CAPS2 can bring people togeth...</td>\n",
       "      <td>0</td>\n",
       "      <td>[one, caps1, caps2, bring, people, together, o...</td>\n",
       "      <td>[just one @caps1 @caps2 can bring people toget...</td>\n",
       "      <td>[one, caps1, caps2, bring, people, together, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21446</th>\n",
       "      <td>4250</td>\n",
       "      <td>22239</td>\n",
       "      <td>8</td>\n",
       "      <td>@CAPS8 Was my first time going to the band ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>[caps8, first, time, going, band, camp, caps1,...</td>\n",
       "      <td>[ @caps8 was my first time going to the band c...</td>\n",
       "      <td>[caps8, first, time, going, band, camp, caps1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21447</th>\n",
       "      <td>4251</td>\n",
       "      <td>22240</td>\n",
       "      <td>8</td>\n",
       "      <td>Laughter is one of the most important things i...</td>\n",
       "      <td>0</td>\n",
       "      <td>[laughter, one, important, things, life, opini...</td>\n",
       "      <td>[laughter is one of the most important things ...</td>\n",
       "      <td>[laughter, one, important, thing, life, opinio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21448</th>\n",
       "      <td>4252</td>\n",
       "      <td>22241</td>\n",
       "      <td>8</td>\n",
       "      <td>A @CAPS1 Of @C...</td>\n",
       "      <td>0</td>\n",
       "      <td>[caps1, caps2, laughter, one, language, speak,...</td>\n",
       "      <td>[                                a @caps1 of @...</td>\n",
       "      <td>[caps1, caps2, laughter, one, language, speak,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21449</th>\n",
       "      <td>4253</td>\n",
       "      <td>22242</td>\n",
       "      <td>8</td>\n",
       "      <td>Good @CAPS1 In @LOCATION1.  Every year a sel...</td>\n",
       "      <td>0</td>\n",
       "      <td>[good, caps1, location1, every, year, select, ...</td>\n",
       "      <td>[  good @caps1 in @location1., every year a se...</td>\n",
       "      <td>[good, caps1, location1, every, year, select, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21450 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  essay_id  essay_set  \\\n",
       "0               0         1          1   \n",
       "1               1         2          1   \n",
       "2               2         3          1   \n",
       "3               3         4          1   \n",
       "4               4         5          1   \n",
       "...           ...       ...        ...   \n",
       "21445        4249     22238          8   \n",
       "21446        4250     22239          8   \n",
       "21447        4251     22240          8   \n",
       "21448        4252     22241          8   \n",
       "21449        4253     22242          8   \n",
       "\n",
       "                                                   essay  ai_generated  \\\n",
       "0      Dear local newspaper, I think effects computer...             0   \n",
       "1      Dear @CAPS1 @CAPS2, I believe that using compu...             0   \n",
       "2      Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...             0   \n",
       "3      Dear Local Newspaper, @CAPS1 I have found that...             0   \n",
       "4      Dear @LOCATION1, I know having computers has a...             0   \n",
       "...                                                  ...           ...   \n",
       "21445  Just One @CAPS1 @CAPS2 can bring people togeth...             0   \n",
       "21446   @CAPS8 Was my first time going to the band ca...             0   \n",
       "21447  Laughter is one of the most important things i...             0   \n",
       "21448                                  A @CAPS1 Of @C...             0   \n",
       "21449    Good @CAPS1 In @LOCATION1.  Every year a sel...             0   \n",
       "\n",
       "                                             word_tokens  \\\n",
       "0      [dear, local, newspaper, think, effects, compu...   \n",
       "1      [dear, caps1, caps2, believe, using, computers...   \n",
       "2      [dear, caps1, caps2, caps3, people, use, compu...   \n",
       "3      [dear, local, newspaper, caps1, found, many, e...   \n",
       "4      [dear, location1, know, computers, positive, e...   \n",
       "...                                                  ...   \n",
       "21445  [one, caps1, caps2, bring, people, together, o...   \n",
       "21446  [caps8, first, time, going, band, camp, caps1,...   \n",
       "21447  [laughter, one, important, things, life, opini...   \n",
       "21448  [caps1, caps2, laughter, one, language, speak,...   \n",
       "21449  [good, caps1, location1, every, year, select, ...   \n",
       "\n",
       "                                         sentence_tokens  \\\n",
       "0      [dear local newspaper, i think effects compute...   \n",
       "1      [dear @caps1 @caps2, i believe that using comp...   \n",
       "2      [dear, @caps1 @caps2 @caps3 more and more peop...   \n",
       "3      [dear local newspaper, @caps1 i have found tha...   \n",
       "4      [dear @location1, i know having computers has ...   \n",
       "...                                                  ...   \n",
       "21445  [just one @caps1 @caps2 can bring people toget...   \n",
       "21446  [ @caps8 was my first time going to the band c...   \n",
       "21447  [laughter is one of the most important things ...   \n",
       "21448  [                                a @caps1 of @...   \n",
       "21449  [  good @caps1 in @location1., every year a se...   \n",
       "\n",
       "                                  lemmatized_word_tokens  \n",
       "0      [dear, local, newspaper, think, effect, comput...  \n",
       "1      [dear, caps1, caps2, believe, using, computer,...  \n",
       "2      [dear, caps1, caps2, caps3, people, use, compu...  \n",
       "3      [dear, local, newspaper, caps1, found, many, e...  \n",
       "4      [dear, location1, know, computer, positive, ef...  \n",
       "...                                                  ...  \n",
       "21445  [one, caps1, caps2, bring, people, together, o...  \n",
       "21446  [caps8, first, time, going, band, camp, caps1,...  \n",
       "21447  [laughter, one, important, thing, life, opinio...  \n",
       "21448  [caps1, caps2, laughter, one, language, speak,...  \n",
       "21449  [good, caps1, location1, every, year, select, ...  \n",
       "\n",
       "[21450 rows x 8 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking worked\n",
    "print(asap_df[\"word_tokens\"].head())\n",
    "print(asap_df[\"sentence_tokens\"].head())\n",
    "print(asap_df[\"lemmatized_word_tokens\"].head())\n",
    "asap_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataframe\n",
    "\n",
    "asap_df.to_excel(\"../cleanData/processedAsap.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       NaN\n",
      "1       NaN\n",
      "2       NaN\n",
      "3       NaN\n",
      "4       NaN\n",
      "         ..\n",
      "21445   NaN\n",
      "21446   NaN\n",
      "21447   NaN\n",
      "21448   NaN\n",
      "21449   NaN\n",
      "Name: word_tokens, Length: 21450, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Removing Special Charactes \n",
    "\n",
    "Remove_Special_CharactersDf=asap_df[\"essay\"].str.replace('\\W', ' ', regex=True)\n",
    "Remove_Special_CharactersDf\n",
    "sentence= Remove_Special_CharactersDf.loc[asap_df.index[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pos_tagger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bm/zz3zwwtj491ggdvwhnvk_wyr0000gn/T/ipykernel_2722/231292653.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# we use our own pos_tagger function to make things simpler to understand.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mwordnet_tagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_tagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bm/zz3zwwtj491ggdvwhnvk_wyr0000gn/T/ipykernel_2722/231292653.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# we use our own pos_tagger function to make things simpler to understand.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mwordnet_tagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_tagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pos_tagger' is not defined"
     ]
    }
   ],
   "source": [
    "# lemmatization is a lot more powerful. It looks beyond word reduction and considers a language’s full vocabulary to\n",
    "# apply a morphological analysis to words, aiming to remove inflectional endings only and to return the base or \n",
    "# dictionary form of a word\n",
    "# Wordnet is a publicly available lexical database of over 200 languages that provides semantic relationships betweenits words\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "  \n",
    "sentence= Remove_Special_CharactersDf.loc[asap_df.index[1]]\n",
    " \n",
    "# tokenize the sentence and find the POS tag for each token\n",
    "pos_tagged = nltk.pos_tag(nltk.word_tokenize(sentence)) \n",
    "  \n",
    "# we use our own pos_tagger function to make things simpler to understand.\n",
    "wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "\n",
    " \n",
    "lemmatized_sentence = []\n",
    "for word, tag in wordnet_tagged:\n",
    "    if tag is None:\n",
    "        # if there is no available tag, append the token as is\n",
    "        lemmatized_sentence.append(word)\n",
    "    else:       \n",
    "        # else use the tag to lemmatize the token\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
    " \n",
    "print(lemmatized_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear CAPS1 CAPS2 believe using computers benefit us many ways like talking becoming friends others websites like facebook mysace Using computers help us find coordibates locations able ourselfs millions information Also computers benefit us helping jobs planning house plan typing NUM1 page report one jobs less writing lets go wonder world technology Using computer help us life talking making friends line Many people myspace facebooks aim benefit us conversations one another Many people believe computers bad make friends never talk fortunate computer help school work social life make friends Computers help us finding locations coordibates millions information online go internet lot know go onto websites MONTH1 help us locations coordinates like LOCATION1 Would rather use computer LOCATION3 supposed vacationing LOCATION2 Million information found internet almost every question computer Would rather easily draw house plan computers take NUM1 hours one hand ugly erazer marks garrenteed find job drawing like Also appling job many workers must write long papers like NUM3 word essay job fits many people know like writing NUM3 words non stopp hours could take hav computer computers needed lot adays hope essay impacted descion computers great machines work day showed mom use computer said greatest invention sense sliced bread go buy computer help chat online friends find locations millions information one click button help self getting job neat prepared printed work boss love\n",
      "Old length:  2288\n",
      "New length:  1491\n"
     ]
    }
   ],
   "source": [
    "   #The words which are generally filtered out before processing a natural language are called stop words\n",
    "#Examples of a few stop words in English are “the”, “a”, “an”, “so”, “what”.\n",
    "#NLTK is a library to play with natural language.The steps to import the library and the English stop words list\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "sw_nltk = stopwords.words('english')\n",
    "\n",
    "words = [word for word in sentence.split() if word.lower() not in sw_nltk]\n",
    "new_text = \" \".join(words)\n",
    "print(new_text)\n",
    "print(\"Old length: \", len(sentence))\n",
    "print(\"New length: \", len(new_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
