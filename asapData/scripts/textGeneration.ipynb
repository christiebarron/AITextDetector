{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "\n",
    "This python notebook generates text based on a series of prompts. \n",
    "\n",
    "## Using Python to generate data with APIs\n",
    "\n",
    "- GPT-2 Medium: 355M parameter version of GPT-2\n",
    "- GPT-2: smallest with 124M parameters.\n",
    "- DistilGPT2: 82 million parameters.\n",
    "- bloom-560m: BigScience Large Open-science Open-access Multilingual Language Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI's Davinci LLM\n",
    "\n",
    "This code:\n",
    "- Loops through API calls that themselves generate multiple ai texts\n",
    "- Saves individual text files\n",
    "- Merges text files to pandas dataframe \n",
    "- Exports dataframe to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing dependencies\n",
    "#if first time, run pip install openai\n",
    "\n",
    "from config import OPENAI_API_KEY\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import openai\n",
    "#openai.organization = \"org-YhptOOA3hPuAmtacppubVTs1\"\n",
    "openai.api_key =  OPENAI_API_KEY #os.getenv(\"OPENAI_API_KEY\")\n",
    "#openai.Model.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model setup  see  https://platform.openai.com/docs/api-reference/completions/create\n",
    "\n",
    "model = \"text-davinci-003\" #the id of the model to include\n",
    "prompt = \"Write a persuasive essay to a newspaper about censorship in libraries. Do you believe that certain materials, such as books, music, movies, and magazines should be removed from the shelves if they are found offensive? \" #the prompt to provide \n",
    "essay_id = 2\n",
    "essay_type = \"persuasive/narrative\"\n",
    "max_tokens = 330 #maximum number of tokens to include\n",
    "n = 50 #number of essays to generate in single API call\n",
    "\n",
    "#count = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (5):\n",
    "\n",
    "    result = openai.Completion.create(model =  model, prompt = prompt, max_tokens = max_tokens, n = n)\n",
    "    \n",
    "    \n",
    "    for x in range(len(result[\"choices\"])):\n",
    "        count = count + 1\n",
    "        text = result[\"choices\"][x][\"text\"]\n",
    "        model = result[\"model\"]\n",
    "\n",
    "        with open(f'../rawData/aiEssays/eid{essay_id}_{model}_{count}.txt', 'w') as f:\n",
    "            f.write(text)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model setup\n",
    "model = \"text-davinci-003\" #the id of the model to include\n",
    "prompt = \"Being patient means that you are understanding and tolerant. A patient person experience difficulties without complaining. Write a true story about a time someone was patient using first person.\" #using first person. #the prompt to provide \n",
    "essay_id = 7\n",
    "essay_type = \"persuasive/narrative\"\n",
    "max_tokens = 330 #maximum number of tokens to include\n",
    "n = 75 #number of essays to generate in single API call\n",
    "\n",
    "#count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (7):\n",
    "\n",
    "    result = openai.Completion.create(model =  model, prompt = prompt, max_tokens = max_tokens, n = n)\n",
    "    \n",
    "    \n",
    "    for x in range(len(result[\"choices\"])):\n",
    "        count = count + 1\n",
    "        text = result[\"choices\"][x][\"text\"]\n",
    "        model = result[\"model\"]\n",
    "\n",
    "        with open(f'../rawData/aiEssays/eid{essay_id}_{model}_{count}.txt', 'w') as f:\n",
    "            f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model setup\n",
    "model = \"text-davinci-003\" #the id of the model to include\n",
    "prompt = \"Write a letter to your local newspaper stating your opinion on the effects computers have on people. Persuade the readers to agree with you.\" #the prompt to provide \n",
    "essay_id = 1\n",
    "essay_type = \"persuasive/narrative\"\n",
    "max_tokens = 300 #maximum number of tokens to include\n",
    "n = 50 #number of essays to generate in single API call\n",
    "count = len(os.listdir('../rawData/aiEssays/')) +2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model setup\n",
    "model = \"text-davinci-003\" #the id of the model to include\n",
    "prompt = \"Laughter is the shortest distance between two people. Laughter is important in any relationship. Write a true story involving laughter in first person.\" #using first person. #the prompt to provide \n",
    "essay_id = 8\n",
    "essay_type = \"persuasive/narrative\"\n",
    "max_tokens = 330 #maximum number of tokens to include\n",
    "n = 75 #number of essays to generate in single API call\n",
    "# We all understand the benefits of laughter. For example, someone once said, “Laughter is the shortest distance between two people.” Many other people believe that laughter is an important part of any relationship. Tell a true story in which laughter was one element or part.\n",
    "#count = 0\n",
    "count = len(os.listdir('../rawData/aiEssays/')) +2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a 200 word essay about the mood created in the memoir 'From Home: The Blueprints of Our Lives' by Narciso Rodriguez.\"\n",
    "model = \"text-davinci-003\" #the id of the model to include\n",
    "essay_id = 5\n",
    "essay_type = \"persuasive/narrative\"\n",
    "max_tokens = 330 #maximum number of tokens to include\n",
    "n = 20 #number of essays to generate in single API call\n",
    "count = len(os.listdir('../rawData/aiEssays/')) +2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2126"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write an essay describing the obstacles the builders of the Empire State Building faced in allowing dirigibles to dock. Use information from 'The Mooring Mast' by Marcia Amidon Lüsted.\"\n",
    "model = \"text-davinci-003\" #the id of the model to include\n",
    "essay_id = 6\n",
    "essay_type = \"persuasive/narrative\"\n",
    "max_tokens = 330 #maximum number of tokens to include\n",
    "n = 20 #number of essays to generate in single API call\n",
    "count = len(os.listdir('../rawData/aiEssays/')) +2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =  \"Why does 'Winter Hibiscus' by Minfong Ho end with 'When they come back in the spring, when the snows melt and the geese return and this hibiscus is budding, then I will take that test again?' Write a 200 word response using examples.\"\n",
    "#model setup\n",
    "model = \"text-davinci-003\" #the id of the model to include\n",
    "essay_id = 4\n",
    "essay_type = \"persuasive/narrative\"\n",
    "max_tokens = 330 #maximum number of tokens to include\n",
    "n = 25 #number of essays to generate in single API call\n",
    "count = len(os.listdir('../rawData/aiEssays/')) +2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model setup\n",
    "model = \"text-davinci-003\" #the id of the model to include\n",
    "prompt = \"Write a 200 word response that explains how the features of the setting affect the cyclist in 'Rough Road Ahead' by Joe Kurmaski.\"\n",
    "essay_id = 3\n",
    "essay_type = \"persuasive/narrative\"\n",
    "max_tokens = 250 #maximum number of tokens to include\n",
    "n = 25 #number of essays to generate in single API call\n",
    "# We all understand the benefits of laughter. For example, someone once said, “Laughter is the shortest distance between two people.” Many other people believe that laughter is an important part of any relationship. Tell a true story in which laughter was one element or part.\n",
    "#count = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (5):\n",
    "\n",
    "    result = openai.Completion.create(model =  model, prompt = prompt, max_tokens = max_tokens, n = n)\n",
    "    \n",
    "    \n",
    "    for x in range(len(result[\"choices\"])):\n",
    "        count = count + 1\n",
    "        text = result[\"choices\"][x][\"text\"]\n",
    "        model = result[\"model\"]\n",
    "\n",
    "        with open(f'../rawData/aiEssays/eid{essay_id}_{model}_{count}.txt', 'w') as f:\n",
    "            f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all the ai essays to a pandas dataframe\n",
    "\n",
    "text_list = [] #create a list to add text files to.\n",
    "files = os.listdir('../rawData/aiEssays/') #get a list of all file names within the directory\n",
    "for f in files:\n",
    "    with open(f'../rawData/aiEssays/{f}', 'r') as f:\n",
    "        text_list.append(f.read())\n",
    "\n",
    "df = pd.DataFrame({'ai_essay': text_list})\n",
    "df.to_excel(\"../cleanData/aiGenerated.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Nomic AI's GPT4All\n",
    "\n",
    "### GPT4All\n",
    "\n",
    "- [Nomic AI](https://github.com/nomic-ai/nomic) needs to be downloaded. \n",
    "    - install nomic, acquire nomic api key.\n",
    "- then clone [GPT4All](https://github.com/nomic-ai/gpt4all)\n",
    "    - then add a file to the chat folder in cloned repository.\n",
    "\n",
    "In terminal write:\n",
    "```\n",
    "pip install nomic\n",
    "nomic login [token]\n",
    "```\n",
    "- then a pompt based on your operating system:\n",
    "M1 Mac/OSX: cd chat;./gpt4all-lora-quantized-OSX-m1\n",
    "Linux: cd chat;./gpt4all-lora-quantized-linux-x86\n",
    "Windows (PowerShell): cd chat;./gpt4all-lora-quantized-win64.exe\n",
    "Intel Mac/OSX: cd chat;./gpt4all-lora-quantized-OSX-intel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nomic.gpt4all import GPT4All\n",
    "m = GPT4All()\n",
    "m.connect()\n",
    "m.prompt('write me a story about a happy computer')\n",
    "\n",
    "#need a M1 mac or linux. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Python Packages Without APIs\n",
    "\n",
    "### Using OpHugging Face's gpt-J-6b model\n",
    "https://huggingface.co/EleutherAI/gpt-j-6B\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install dependencies \n",
    "\n",
    "#if first time, run:   pip install transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Hugging Face's gpt-j-6b\n",
    "#note that may need to install huggingface module(s): https://huggingface.co/welcome\n",
    "\n",
    "#code from https://huggingface.co/EleutherAI/gpt-j-6B\n",
    "\n",
    "#BE AWARE THIS CODE TAKES 20-40 MINUTES TO RUN\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "\n",
    "#consider isolating to one file or scaling down. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BigScience's T0pp model\n",
    "https://huggingface.co/bigscience/T0pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/T0pp\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\")\n",
    "\n",
    "inputs = tokenizer.encode(\"Is this review positive or negative? Review: this is the best cast iron skillet you will ever buy\", return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using NLP Cloud's Instruct version of GPT-J\n",
    " #https://huggingface.co/nlpcloud/instruct-gpt-j-fp16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "#Example code from Medium.com \n",
    " #https://medium.com/the-side-hustle-club/generate-text-content-with-7-lines-of-python-and-ai-4031ef5da5bf\n",
    "\n",
    "# CAUTION THIS CODE TAKES 60+ MINUTES TO RUN\n",
    "from transformers import pipeline\n",
    "\n",
    "# Select a task\n",
    "task = \"text-generation\"\n",
    "\n",
    "# Select a pre-trained model to use\n",
    "model_name = \"EleutherAI/gpt-j-6B\"\n",
    "\n",
    "# Load Model from HuggingFace Hub\n",
    "text_gen = pipeline(task, model=model_name) \n",
    " #originally  text_gen = pipeline(task, model=model_name) \n",
    "\n",
    "# Define a baseline prompt for the generator\n",
    "base = \"I really like to play football, this sport is \"\n",
    "\n",
    "# Get generator's response\n",
    "res = text_gen(base, max_length=200)\n",
    "print(res)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d508eba07fb8b659bf0eed10b1d2de2af8181f46a6c79a675eab472ca0c71a9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
