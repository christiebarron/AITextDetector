{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6c6a697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk.tree import Tree\n",
    "import spacy\n",
    "\n",
    "\n",
    "# Initialize spaCy English model\n",
    "nlp_spacy = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc5dd15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract lexical features\n",
    "def extract_lexical_features(text):\n",
    "    # ... your extract_lexical_features function implementation ...\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    total_word_count = len(words)\n",
    "    avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "    avg_sentence_length = sum(len(sentence.split()) for sentence in sentences) / len(sentences)\n",
    "    word_counts = Counter(words)\n",
    "    TTR = len(word_counts) / len(words)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_word_count = sum(1 for word in words if word.lower() in stop_words)\n",
    "    unique_word_count = sum(1 for _, count in word_counts.items() if count == 1)\n",
    "    word_freq = word_counts\n",
    "    bigram_freq = Counter(ngrams(words, 2))\n",
    "    trigram_freq = Counter(ngrams(words, 3))\n",
    "    rare_word_count = sum(1 for _, count in word_counts.items() if count == 1)\n",
    "\n",
    "    return {\n",
    "        'total_word_count': total_word_count,\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'TTR': TTR,\n",
    "        'stop_word_count': stop_word_count,\n",
    "        'unique_word_count': unique_word_count,\n",
    "        'word_freq': word_freq,\n",
    "        'bigram_freq': bigram_freq,\n",
    "        'trigram_freq': trigram_freq,\n",
    "        'rare_word_count': rare_word_count\n",
    "    }\n",
    "\n",
    "#load data from excel file and save as list\n",
    "merged_df = pd.read_excel('../cleanData/processedAsap.xlsx')\n",
    "all_essays = merged_df['essay'].tolist()\n",
    "\n",
    "# Extract lexical features from AI-generated and human-written essays\n",
    "all_lexical_features = [extract_lexical_features(essay) for essay in all_essays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c23d287",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([merged_df, pd.DataFrame(all_lexical_features)], axis = 1)#.to_excel(\"../cleanData/featuresAsap.xlsx\")\n",
    "merged_df.to_excel(\"../cleanData/featuresAsap.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b99076",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to extract syntactic features\n",
    "def extract_syntactic_features(text):\n",
    "    # ... your extract_syntactic_features function implementation ...\n",
    "    doc = nlp_spacy(text)\n",
    "\n",
    "    # Calculate average sentence length\n",
    "    sentence_lengths = [len(sent) for sent in doc.sents]\n",
    "    avg_sentence_length = np.mean(sentence_lengths)\n",
    "\n",
    "    # Calculate parse tree depth\n",
    "    def calc_tree_depth(sent):\n",
    "        root = [token for token in sent if token.head == token][0]\n",
    "        return max([len(list(token.ancestors)) for token in sent])\n",
    "\n",
    "    tree_depths = [calc_tree_depth(sent) for sent in doc.sents]\n",
    "    avg_parse_tree_depth = np.mean(tree_depths)\n",
    "    parse_tree_depth_variation = np.std(tree_depths)\n",
    "\n",
    "    return {\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'avg_parse_tree_depth': avg_parse_tree_depth,\n",
    "        'parse_tree_depth_variation': parse_tree_depth_variation,\n",
    "    }\n",
    "\n",
    "\n",
    "# Extract syntactic features from AI-generated and human-written essays\n",
    "all_syntactic_features = [extract_syntactic_features(essay) for essay in all_essays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873a75b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([merged_df, pd.DataFrame(all_syntactic_features)], axis = 1)#.to_excel(\"../cleanData/features.Asap.xlsx\")\n",
    "merged_df.to_excel(\"../cleanData/featuresAsap.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4793bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine lexical and syntactic features\n",
    "# def combined_features(text):\n",
    "#     lexical = extract_lexical_features(text)\n",
    "#     syntactic = extract_syntactic_features(text)\n",
    "#     return {**lexical, **syntactic}\n",
    "\n",
    "# # Extract combined features for AI-generated and human-written essays\n",
    "# all_combined_features = [combined_features(essay) for essay in all_essays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5864d072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stylistic Features\n",
    "def extract_stylistic_features(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    num_sentences = len(sentences)\n",
    "    \n",
    "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "    pos_tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "    \n",
    "    num_adjectives = sum(sum(1 for word, pos in sentence if pos.startswith('JJ')) for sentence in pos_tagged_sentences)\n",
    "    num_adverbs = sum(sum(1 for word, pos in sentence if pos.startswith('RB')) for sentence in pos_tagged_sentences)\n",
    "    num_verbs = sum(sum(1 for word, pos in sentence if pos.startswith('VB')) for sentence in pos_tagged_sentences)\n",
    "    num_nouns = sum(sum(1 for word, pos in sentence if pos.startswith('NN')) for sentence in pos_tagged_sentences)\n",
    "\n",
    "    avg_adjectives_per_sentence = num_adjectives / num_sentences\n",
    "    avg_adverbs_per_sentence = num_adverbs / num_sentences\n",
    "    avg_verbs_per_sentence = num_verbs / num_sentences\n",
    "    avg_nouns_per_sentence = num_nouns / num_sentences\n",
    "    \n",
    "    return {\n",
    "        'avg_adjectives_per_sentence': avg_adjectives_per_sentence,\n",
    "        'avg_adverbs_per_sentence': avg_adverbs_per_sentence,\n",
    "        'avg_verbs_per_sentence': avg_verbs_per_sentence,\n",
    "        'avg_nouns_per_sentence': avg_nouns_per_sentence,\n",
    "    }\n",
    "\n",
    "# Extract stylistic features from AI-generated and human-written essays\n",
    "all_stylistic_features = [extract_stylistic_features(essay) for essay in all_essays]\n",
    "\n",
    "import string\n",
    "\n",
    "def count_punctuation(text):\n",
    "    punctuation_count = sum(1 for char in text if char in string.punctuation)\n",
    "    punct_length = sum(1 for char in text)\n",
    "    punctuation_proportion = punctuation_count / punct_length\n",
    "    return {\"punctuation_proportion\" :punctuation_proportion}\n",
    "\n",
    "all_avg_punctuation = [count_punctuation(essay) for essay in all_essays]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55d995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([merged_df, pd.DataFrame(all_stylistic_features)], axis = 1)#   pd.DataFrame(all_avg_punctuation)\n",
    "merged_df.to_excel(\"../cleanData/featuresAsap.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce3b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Initialize spaCy English model\n",
    "nlp_spacy = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to count passive sentences\n",
    "def count_passive_sentences(text):\n",
    "    passive_sentences = 0\n",
    "    doc = nlp_spacy(text)\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'nsubjpass':\n",
    "            passive_sentences += 1\n",
    "    return passive_sentences\n",
    "\n",
    "# Function to calculate readability scores\n",
    "#from readability import Readability\n",
    "#from readability.exceptions import ReadabilityException\n",
    "\n",
    "\n",
    "import textstat\n",
    "\n",
    "# Function to calculate readability scores\n",
    "def readability_scores(text):\n",
    "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
    "    flesch_kincaid_grade_level = textstat.text_standard(text, float_output=True)\n",
    "    smog_index = textstat.smog_index(text)\n",
    "    return {\n",
    "        \"flesch_reading_ease\" : flesch_reading_ease, \n",
    "        \"flesch_kincaid_grade_level\" : flesch_kincaid_grade_level, \n",
    "        \"smog_index\" : smog_index}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to calculate sentiment analysis scores\n",
    "def sentiment_analysis_scores(text):\n",
    "    sentiment = TextBlob(text)\n",
    "    return {\n",
    "        \"sentiment_polarity\" : sentiment.polarity, \n",
    "        \"sentiment.subjectivity\" : sentiment.subjectivity}\n",
    "\n",
    "# Calculate new features for AI-generated and human-written essays\n",
    "all_passive_sentences = [count_passive_sentences(essay) for essay in all_essays]\n",
    "    #ai_generated_passive_sentences = [count_passive_sentences(essay) for essay in ai_generated_essays]\n",
    "    #human_written_passive_sentences = [count_passive_sentences(essay) for essay in human_written_essays]\n",
    "\n",
    "\n",
    "all_readibility_scores = [readability_scores(essay) for essay in all_essays]\n",
    "    #ai_generated_readability_scores = [readability_scores(essay) for essay in ai_generated_essays]\n",
    "    #human_written_readability_scores = [readability_scores(essay) for essay in human_written_essays]\n",
    "\n",
    "all_sentiment_scores = [sentiment_analysis_scores(essay) for essay in all_essays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725723a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the extracted features with the original data\n",
    "merged_df = pd.concat([merged_df, pd.DataFrame(all_passive_sentences), pd.DataFrame(all_readibility_scores), pd.DataFrame(all_sentiment_scores)], axis = 1) #pd.DataFrame(all_lexical_features), pd.DataFrame(all_syntactic_features), pd.DataFrame(all_stylistic_features)\n",
    "\n",
    "#save as excel document\n",
    "merged_df.to_excel(\"../cleanData/featuresAsap.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b770cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate essay perplexity\n",
    "\n",
    "#install dependencies\n",
    "#install dependencies\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline #function used to prepare the tokenized text accordingly\n",
    "from nltk.lm import MLE\n",
    "from nltk.util import bigrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "import string\n",
    "\n",
    "#install the “popular” subset of NLTK data, on the command line type\n",
    "#python -m nltk.downloader popular\n",
    "import nltk.tokenize\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.lm import MLE #import a maximum likelihood estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f68b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to tokenize text while keeping stop words\n",
    "def clean_text_keep_stopword(text):\n",
    "    sentences = nltk.sent_tokenize(text) #create sentence tokens (not cleaned)\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation]) #remove punctuation\n",
    "    tokens = nltk.tokenize.word_tokenize(text) #tokenize\n",
    "    return(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d0a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing for perplexity\n",
    "perplexty_df = merged_df.loc[:, [\"essay_id\", \"ai_llm\", \"essay\"]]\n",
    "\n",
    "#run the word tokenized function through every row of text\n",
    "perplexty_df['word_token_with_stopword'] = perplexty_df[\"essay\"].apply(lambda x: clean_text_keep_stopword(x.lower())) \n",
    "\n",
    "#use padded_everygram_pipeline() to preprocess the tokenized data \n",
    "train, vocab = padded_everygram_pipeline(2, perplexty_df['word_token_with_stopword'])\n",
    "\n",
    "#import a maximum likelihood estimator\n",
    "from nltk.lm import MLE \n",
    "lm = MLE(2) #use MLE function to create an empty vocabulary\n",
    "\n",
    " #fit the MLE model to the preprocessed data\n",
    "lm.fit(train, vocab)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b136f4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use a list comprehension to run two functions on the word tokens with stopwords for each essay:\n",
    " #1) create a series of bigrams based on the text, 2) calculate entropy\n",
    "merged_df[\"perplexity\"] = perplexty_df['word_token_with_stopword'].apply(lambda x: lm.perplexity(list(bigrams(pad_both_ends(x, n=2)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a821b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_excel(\"../cleanData/featuresAsap.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
