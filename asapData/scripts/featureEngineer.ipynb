{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features to add\n",
    "- essay \"perplexity\" and variance in perplexity.  Seems to be the most important predictor in the literature.\n",
    "\n",
    "### Additional features to add (lower priority)\n",
    "- length of writing\n",
    "- average length of sentence\n",
    "- average complexity of a word\n",
    "- percentage of text that is punctuation\n",
    "- percentage of text that is commas (or average # of commas per sentence)\n",
    "- proportion of text that are nouns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "asap_df = pd.read_excel(\"../cleanData/mergedAsap.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code defines a function extract_features that takes a text as input and returns a dictionary of features that could be characteristic of text generated by AI. \n",
    "#In this example, the function checks for features related to sentence structure and syntax, distinctive vocabulary and word choice, overall coherence and flow, and use of examples and evidence.\n",
    "#Need to install nltk before we can proceed\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "​\n",
    "# Define a function to extract features from a given text\n",
    "def extract_features(text):\n",
    "    features = {}\n",
    "    \n",
    "    # Tokenize the text into sentences and words\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Check for sentence structure and syntax patterns\n",
    "    if any('.' in sentence for sentence in sentences):\n",
    "        features['uses_period'] = True\n",
    "    if any('!' in sentence for sentence in sentences):\n",
    "        features['uses_exclamation'] = True\n",
    "    if any('?' in sentence for sentence in sentences):\n",
    "        features['uses_question'] = True\n",
    "        \n",
    "    # Check for distinctive vocabulary and word choice\n",
    "    common_words = ['computer', 'technology', 'internet']\n",
    "    common_word_count = sum(1 for word in words if word.lower() in common_words)\n",
    "    if common_word_count > 2:\n",
    "        features['uses_common_words'] = True\n",
    "        \n",
    "    # Check for overall coherence and flow\n",
    "    if len(sentences) > 3:\n",
    "        sentence_lengths = [len(sentence) for sentence in sentences]\n",
    "        if max(sentence_lengths) > 100:\n",
    "            features['long_sentences'] = True\n",
    "    \n",
    "    # Check for use of examples and evidence\n",
    "    if any('for example' in sentence for sentence in sentences):\n",
    "        features['uses_example'] = True\n",
    "    if any('according to' in sentence for sentence in sentences):\n",
    "        features['uses_evidence'] = True\n",
    "    \n",
    "    return features\n",
    "Coll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example code in Python using NLTK to calculate the average sentence length in a given text:\n",
    "#Need to install natural language toolkit to proceed; here's the link https://www.nltk.org/data.html\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "​\n",
    "# Define a function to calculate the average sentence length\n",
    "def avg_sentence_length(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Calculate the total number of words and sentences\n",
    "    num_words = sum(len(sentence.split()) for sentence in sentences)\n",
    "    num_sentences = len(sentences)\n",
    "    \n",
    "    # Calculate the average sentence length\n",
    "    avg_length = num_words / num_sentences\n",
    "    \n",
    "    return avg_length\n",
    "​\n",
    "#You can use this function by passing in a string of text as an argument:\n",
    "text = \"More and more people use computers, but not everyone agrees that this benefits society. Those who support advances in technology believe that computers have a positive effect on people. They teach hand-eye coordination, give people the ability to learn about faraway places and people, and even allow people to talk online with other people. Others have different ideas. Some experts are concerned that people are spending too much time on their computers and less time exercising, enjoying nature, and interacting with family and friends.\"\n",
    "​\n",
    "avg_length = avg_sentence_length(text)\n",
    "print(\"Average sentence length:\", avg_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
