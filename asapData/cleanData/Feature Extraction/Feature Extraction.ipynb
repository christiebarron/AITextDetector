{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6c6a697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk.tree import Tree\n",
    "import spacy\n",
    "\n",
    "\n",
    "# Initialize spaCy English model\n",
    "nlp_spacy = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc5dd15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract lexical features\n",
    "def extract_lexical_features(text):\n",
    "    # ... your extract_lexical_features function implementation ...\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    total_word_count = len(words)\n",
    "    avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "    avg_sentence_length = sum(len(sentence.split()) for sentence in sentences) / len(sentences)\n",
    "    word_counts = Counter(words)\n",
    "    TTR = len(word_counts) / len(words)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_word_count = sum(1 for word in words if word.lower() in stop_words)\n",
    "    unique_word_count = sum(1 for _, count in word_counts.items() if count == 1)\n",
    "    word_freq = word_counts\n",
    "    bigram_freq = Counter(ngrams(words, 2))\n",
    "    trigram_freq = Counter(ngrams(words, 3))\n",
    "    rare_word_count = sum(1 for _, count in word_counts.items() if count == 1)\n",
    "\n",
    "    return {\n",
    "        'total_word_count': total_word_count,\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'TTR': TTR,\n",
    "        'stop_word_count': stop_word_count,\n",
    "        'unique_word_count': unique_word_count,\n",
    "        'word_freq': word_freq,\n",
    "        'bigram_freq': bigram_freq,\n",
    "        'trigram_freq': trigram_freq,\n",
    "        'rare_word_count': rare_word_count\n",
    "    }\n",
    "\n",
    "\n",
    "ai_generated_df = pd.read_excel('aiGenerated.xlsx',nrows=50)\n",
    "ai_generated_essays = ai_generated_df['ai_essay'].tolist()\n",
    "\n",
    "# Load human-written essays from an Excel file\n",
    "human_written_df = pd.read_excel('training_set_rel3.xlsx',nrows=50)\n",
    "human_written_essays = human_written_df['essay'].tolist()\n",
    "\n",
    "# Extract lexical features from AI-generated and human-written essays\n",
    "ai_generated_features = [extract_lexical_features(essay) for essay in ai_generated_essays]\n",
    "human_written_features = [extract_lexical_features(essay) for essay in human_written_essays]\n",
    "\n",
    "\n",
    "# Calculate average values of some lexical features for both AI-generated and human-written essays\n",
    "def average_feature_value(features, feature_key):\n",
    "    return sum(feature[feature_key] for feature in features) / len(features)\n",
    "\n",
    "ai_avg_word_length = average_feature_value(ai_generated_features, 'avg_word_length')\n",
    "ai_avg_TTR = average_feature_value(ai_generated_features, 'TTR')\n",
    "ai_avg_stop_word_count = average_feature_value(ai_generated_features, 'stop_word_count')\n",
    "\n",
    "ai_avg_sentence_length = average_feature_value(ai_generated_features, 'avg_sentence_length')\n",
    "human_avg_sentence_length = average_feature_value(human_written_features, 'avg_sentence_length')\n",
    "human_avg_word_length = average_feature_value(human_written_features, 'avg_word_length')\n",
    "human_avg_TTR = average_feature_value(human_written_features, 'TTR')\n",
    "human_avg_stop_word_count = average_feature_value(human_written_features, 'stop_word_count')\n",
    "\n",
    "# Calculate average values of the total word count for both AI-generated and human-written essays\n",
    "ai_avg_total_word_count = average_feature_value(ai_generated_features, 'total_word_count')\n",
    "human_avg_total_word_count = average_feature_value(human_written_features, 'total_word_count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6b99076",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to extract syntactic features\n",
    "def extract_syntactic_features(text):\n",
    "    # ... your extract_syntactic_features function implementation ...\n",
    "    doc = nlp_spacy(text)\n",
    "\n",
    "    # Calculate average sentence length\n",
    "    sentence_lengths = [len(sent) for sent in doc.sents]\n",
    "    avg_sentence_length = np.mean(sentence_lengths)\n",
    "\n",
    "    # Calculate parse tree depth\n",
    "    def calc_tree_depth(sent):\n",
    "        root = [token for token in sent if token.head == token][0]\n",
    "        return max([len(list(token.ancestors)) for token in sent])\n",
    "\n",
    "    tree_depths = [calc_tree_depth(sent) for sent in doc.sents]\n",
    "    avg_parse_tree_depth = np.mean(tree_depths)\n",
    "    parse_tree_depth_variation = np.std(tree_depths)\n",
    "\n",
    "    return {\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'avg_parse_tree_depth': avg_parse_tree_depth,\n",
    "        'parse_tree_depth_variation': parse_tree_depth_variation,\n",
    "    }\n",
    "\n",
    "\n",
    "# Extract syntactic features from AI-generated and human-written essays\n",
    "ai_generated_syntactic_features = [extract_syntactic_features(essay) for essay in ai_generated_essays]\n",
    "human_written_syntactic_features = [extract_syntactic_features(essay) for essay in human_written_essays]\n",
    "\n",
    "# Compare the syntactic features\n",
    "ai_avg_sentence_length = np.mean([features['avg_sentence_length'] for features in ai_generated_syntactic_features])\n",
    "human_avg_sentence_length = np.mean([features['avg_sentence_length'] for features in human_written_syntactic_features])\n",
    "\n",
    "ai_avg_parse_tree_depth = np.mean([features['avg_parse_tree_depth'] for features in ai_generated_syntactic_features])\n",
    "human_avg_parse_tree_depth = np.mean([features['avg_parse_tree_depth'] for features in human_written_syntactic_features])\n",
    "\n",
    "ai_parse_tree_depth_variation = np.mean([features['parse_tree_depth_variation'] for features in ai_generated_syntactic_features])\n",
    "human_parse_tree_depth_variation = np.mean([features['parse_tree_depth_variation'] for features in human_written_syntactic_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4793bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine lexical and syntactic features\n",
    "def combined_features(text):\n",
    "    lexical = extract_lexical_features(text)\n",
    "    syntactic = extract_syntactic_features(text)\n",
    "    return {**lexical, **syntactic}\n",
    "\n",
    "# Extract combined features for AI-generated and human-written essays\n",
    "ai_generated_combined_features = [combined_features(essay) for essay in ai_generated_essays]\n",
    "human_written_combined_features = [combined_features(essay) for essay in human_written_essays]\n",
    "\n",
    "# Create a DataFrame for AI-generated essays\n",
    "ai_generated_df = pd.DataFrame(ai_generated_combined_features)\n",
    "ai_generated_df['type'] = 'AI-generated'\n",
    "\n",
    "# Create a DataFrame for human-written essays\n",
    "human_written_df = pd.DataFrame(human_written_combined_features)\n",
    "human_written_df['type'] = 'Human-written'\n",
    "\n",
    "# Calculate the average values of features for both AI-generated and human-written essays\n",
    "def average_feature_value(features, feature_key):\n",
    "    return sum(feature[feature_key] for feature in features) / len(features)\n",
    "\n",
    "# Define a list of feature keys to extract from the combined features\n",
    "feature_keys = [\n",
    "    'total_word_count',\n",
    "    'avg_word_length',\n",
    "    'avg_sentence_length',\n",
    "    'TTR',\n",
    "    'stop_word_count',\n",
    "    'unique_word_count',\n",
    "    'rare_word_count',\n",
    "    'avg_parse_tree_depth',\n",
    "    'parse_tree_depth_variation'\n",
    "]\n",
    "\n",
    "# Calculate the average values for each feature\n",
    "ai_generated_avgs = [average_feature_value(ai_generated_combined_features, key) for key in feature_keys]\n",
    "human_written_avgs = [average_feature_value(human_written_combined_features, key) for key in feature_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5864d072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stylistic Features\n",
    "def extract_stylistic_features(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    num_sentences = len(sentences)\n",
    "    \n",
    "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "    pos_tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "    \n",
    "    num_adjectives = sum(sum(1 for word, pos in sentence if pos.startswith('JJ')) for sentence in pos_tagged_sentences)\n",
    "    num_adverbs = sum(sum(1 for word, pos in sentence if pos.startswith('RB')) for sentence in pos_tagged_sentences)\n",
    "    num_verbs = sum(sum(1 for word, pos in sentence if pos.startswith('VB')) for sentence in pos_tagged_sentences)\n",
    "    num_nouns = sum(sum(1 for word, pos in sentence if pos.startswith('NN')) for sentence in pos_tagged_sentences)\n",
    "\n",
    "    avg_adjectives_per_sentence = num_adjectives / num_sentences\n",
    "    avg_adverbs_per_sentence = num_adverbs / num_sentences\n",
    "    avg_verbs_per_sentence = num_verbs / num_sentences\n",
    "    avg_nouns_per_sentence = num_nouns / num_sentences\n",
    "    \n",
    "    return {\n",
    "        'avg_adjectives_per_sentence': avg_adjectives_per_sentence,\n",
    "        'avg_adverbs_per_sentence': avg_adverbs_per_sentence,\n",
    "        'avg_verbs_per_sentence': avg_verbs_per_sentence,\n",
    "        'avg_nouns_per_sentence': avg_nouns_per_sentence,\n",
    "    }\n",
    "\n",
    "# Extract stylistic features from AI-generated and human-written essays\n",
    "ai_generated_stylistic_features = [extract_stylistic_features(essay) for essay in ai_generated_essays]\n",
    "human_written_stylistic_features = [extract_stylistic_features(essay) for essay in human_written_essays]\n",
    "\n",
    "# Calculate average values of stylistic features for both AI-generated and human-written essays\n",
    "ai_avg_adjectives_per_sentence = average_feature_value(ai_generated_stylistic_features, 'avg_adjectives_per_sentence')\n",
    "ai_avg_adverbs_per_sentence = average_feature_value(ai_generated_stylistic_features, 'avg_adverbs_per_sentence')\n",
    "ai_avg_verbs_per_sentence = average_feature_value(ai_generated_stylistic_features, 'avg_verbs_per_sentence')\n",
    "ai_avg_nouns_per_sentence = average_feature_value(ai_generated_stylistic_features, 'avg_nouns_per_sentence')\n",
    "\n",
    "human_avg_adjectives_per_sentence = average_feature_value(human_written_stylistic_features, 'avg_adjectives_per_sentence')\n",
    "human_avg_adverbs_per_sentence = average_feature_value(human_written_stylistic_features, 'avg_adverbs_per_sentence')\n",
    "human_avg_verbs_per_sentence = average_feature_value(human_written_stylistic_features, 'avg_verbs_per_sentence')\n",
    "human_avg_nouns_per_sentence = average_feature_value(human_written_stylistic_features, 'avg_nouns_per_sentence')\n",
    "\n",
    "import string\n",
    "\n",
    "def count_punctuation(text):\n",
    "    punctuation_count = sum(1 for char in text if char in string.punctuation)\n",
    "    return punctuation_count\n",
    "\n",
    "def average_value(values):\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "ai_avg_punctuation = average_value([count_punctuation(essay) for essay in ai_generated_essays])\n",
    "human_avg_punctuation = average_value([count_punctuation(essay) for essay in human_written_essays])\n",
    "\n",
    "\n",
    "# ai_avg_punctuation = average_feature_value([(count_punctuation(essay)) for essay in ai_generated_essays])\n",
    "# human_avg_punctuation = average_feature_value([(count_punctuation(essay)) for essay in human_written_essays])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3dc250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = {\n",
    "    'Feature': ['Total Word Count', \n",
    "                'Average Word Length', \n",
    "                'Average Sentence Length', \n",
    "                'Type-Token Ratio', \n",
    "                'Stop Word Count', \n",
    "                'Average Parse Tree Depth', \n",
    "                'Parse Tree Depth Variation', \n",
    "                'Average Adjectives per Sentence', \n",
    "                'Average Adverbs per Sentence', \n",
    "                'Average Verbs per Sentence', \n",
    "                'Average Nouns per Sentence', \n",
    "                'Average Punctuation Marks'],\n",
    "    'AI-Generated': [ai_avg_total_word_count, \n",
    "                     ai_avg_word_length, ai_avg_sentence_length, \n",
    "                     ai_avg_TTR, ai_avg_stop_word_count, \n",
    "                     ai_avg_parse_tree_depth, \n",
    "                     ai_parse_tree_depth_variation, \n",
    "                     ai_avg_adjectives_per_sentence, \n",
    "                     ai_avg_adverbs_per_sentence, \n",
    "                     ai_avg_verbs_per_sentence, \n",
    "                     ai_avg_nouns_per_sentence, \n",
    "                     ai_avg_punctuation],\n",
    "    'Human-Written': [human_avg_total_word_count, \n",
    "                      human_avg_word_length, \n",
    "                      human_avg_sentence_length, \n",
    "                      human_avg_TTR, \n",
    "                      human_avg_stop_word_count, \n",
    "                      human_avg_parse_tree_depth, \n",
    "                      human_parse_tree_depth_variation, \n",
    "                      human_avg_adjectives_per_sentence, \n",
    "                      human_avg_adverbs_per_sentence, \n",
    "                      human_avg_verbs_per_sentence, \n",
    "                      human_avg_nouns_per_sentence, \n",
    "                      human_avg_punctuation],\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# # Save the updated comparison DataFrame to an Excel file\n",
    "# comparison_df.to_excel('feature_comparison.xlsx', index=False)\n",
    "import openpyxl\n",
    "# Save the comparison DataFrame to an Excel file\n",
    "file_name = 'feature_comparison.xlsx'\n",
    "comparison_df.to_excel(file_name, index=False)\n",
    "\n",
    "# Autofit the column widths using openpyxl\n",
    "workbook = openpyxl.load_workbook(file_name)\n",
    "worksheet = workbook.active\n",
    "\n",
    "for column_cells in worksheet.columns:\n",
    "    length = max(len(str(cell.value)) for cell in column_cells)\n",
    "    worksheet.column_dimensions[column_cells[0].column_letter].width = length\n",
    "\n",
    "workbook.save(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4ce3b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>AI-Generated</th>\n",
       "      <th>Human-Written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Word Count</td>\n",
       "      <td>247.460000</td>\n",
       "      <td>384.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Average Word Length</td>\n",
       "      <td>4.194879</td>\n",
       "      <td>3.994626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Average Sentence Length</td>\n",
       "      <td>21.575748</td>\n",
       "      <td>18.759387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Type-Token Ratio</td>\n",
       "      <td>0.529432</td>\n",
       "      <td>0.472312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stop Word Count</td>\n",
       "      <td>111.480000</td>\n",
       "      <td>167.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Avg Parse Tree Depth</td>\n",
       "      <td>5.554981</td>\n",
       "      <td>4.802128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Parse Tree Depth Variation</td>\n",
       "      <td>1.851215</td>\n",
       "      <td>1.676963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Punctuation Count</td>\n",
       "      <td>24.340000</td>\n",
       "      <td>47.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Passive Sentences</td>\n",
       "      <td>1.680000</td>\n",
       "      <td>1.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Flesch Reading Ease</td>\n",
       "      <td>64.923400</td>\n",
       "      <td>73.179600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SMOG Index</td>\n",
       "      <td>10.060000</td>\n",
       "      <td>7.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sentiment Polarity</td>\n",
       "      <td>0.109202</td>\n",
       "      <td>0.187748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sentiment Subjectivity</td>\n",
       "      <td>0.521389</td>\n",
       "      <td>0.465000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Feature  AI-Generated  Human-Written\n",
       "0             Total Word Count    247.460000     384.060000\n",
       "1          Average Word Length      4.194879       3.994626\n",
       "2      Average Sentence Length     21.575748      18.759387\n",
       "3             Type-Token Ratio      0.529432       0.472312\n",
       "4              Stop Word Count    111.480000     167.880000\n",
       "5         Avg Parse Tree Depth      5.554981       4.802128\n",
       "6   Parse Tree Depth Variation      1.851215       1.676963\n",
       "7            Punctuation Count     24.340000      47.760000\n",
       "8            Passive Sentences      1.680000       1.080000\n",
       "9          Flesch Reading Ease     64.923400      73.179600\n",
       "10                  SMOG Index     10.060000       7.900000\n",
       "11          Sentiment Polarity      0.109202       0.187748\n",
       "12      Sentiment Subjectivity      0.521389       0.465000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Initialize spaCy English model\n",
    "nlp_spacy = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to count passive sentences\n",
    "def count_passive_sentences(text):\n",
    "    passive_sentences = 0\n",
    "    doc = nlp_spacy(text)\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'nsubjpass':\n",
    "            passive_sentences += 1\n",
    "    return passive_sentences\n",
    "\n",
    "# Function to calculate readability scores\n",
    "from readability import Readability\n",
    "from readability.exceptions import ReadabilityException\n",
    "\n",
    "\n",
    "import textstat\n",
    "\n",
    "# Function to calculate readability scores\n",
    "def readability_scores(text):\n",
    "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
    "    flesch_kincaid_grade_level = textstat.text_standard(text, float_output=True)\n",
    "    smog_index = textstat.smog_index(text)\n",
    "    return flesch_reading_ease, flesch_kincaid_grade_level, smog_index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to calculate sentiment analysis scores\n",
    "def sentiment_analysis_scores(text):\n",
    "    sentiment = TextBlob(text)\n",
    "    return sentiment.polarity, sentiment.subjectivity\n",
    "\n",
    "# Calculate new features for AI-generated and human-written essays\n",
    "ai_generated_passive_sentences = [count_passive_sentences(essay) for essay in ai_generated_essays]\n",
    "human_written_passive_sentences = [count_passive_sentences(essay) for essay in human_written_essays]\n",
    "\n",
    "ai_generated_readability_scores = [readability_scores(essay) for essay in ai_generated_essays]\n",
    "human_written_readability_scores = [readability_scores(essay) for essay in human_written_essays]\n",
    "\n",
    "ai_generated_sentiment_scores = [sentiment_analysis_scores(essay) for essay in ai_generated_essays]\n",
    "human_written_sentiment_scores = [sentiment_analysis_scores(essay) for essay in human_written_essays]\n",
    "\n",
    "# Calculate average values for the new features\n",
    "ai_avg_passive_sentences = np.mean(ai_generated_passive_sentences)\n",
    "human_avg_passive_sentences = np.mean(human_written_passive_sentences)\n",
    "\n",
    "ai_avg_flesch_reading_ease = np.mean([score[0] for score in ai_generated_readability_scores])\n",
    "human_avg_flesch_reading_ease = np.mean([score[0] for score in human_written_readability_scores])\n",
    "\n",
    "ai_avg_smog_index = np.mean([score[1] for score in ai_generated_readability_scores])\n",
    "human_avg_smog_index = np.mean([score[1] for score in human_written_readability_scores])\n",
    "\n",
    "ai_avg_polarity = np.mean([score[0] for score in ai_generated_sentiment_scores])\n",
    "human_avg_polarity = np.mean([score[0] for score in human_written_sentiment_scores])\n",
    "\n",
    "ai_avg_subjectivity = np.mean([score[1] for score in ai_generated_sentiment_scores])\n",
    "human_avg_subjectivity = np.mean([score[1] for score in human_written_sentiment_scores])\n",
    "\n",
    "# Update comparison_data with the new features\n",
    "comparison_data = {\n",
    "    'Feature': ['Total Word Count', 'Average Word Length', 'Average Sentence Length', 'Type-Token Ratio', 'Stop Word Count', 'Avg Parse Tree Depth', 'Parse Tree Depth Variation', 'Punctuation Count', 'Passive Sentences', 'Flesch Reading Ease', 'SMOG Index', 'Sentiment Polarity', 'Sentiment Subjectivity'],\n",
    "    'AI-Generated': [ai_avg_total_word_count, ai_avg_word_length, ai_avg_sentence_length, ai_avg_TTR, ai_avg_stop_word_count, ai_avg_parse_tree_depth, ai_parse_tree_depth_variation, ai_avg_punctuation, ai_avg_passive_sentences, ai_avg_flesch_reading_ease, ai_avg_smog_index, ai_avg_polarity, ai_avg_subjectivity],\n",
    "    'Human-Written': [human_avg_total_word_count, human_avg_word_length, human_avg_sentence_length, human_avg_TTR, human_avg_stop_word_count, human_avg_parse_tree_depth, human_parse_tree_depth_variation, human_avg_punctuation, human_avg_passive_sentences, human_avg_flesch_reading_ease, human_avg_smog_index, human_avg_polarity, human_avg_subjectivity],\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725723a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
