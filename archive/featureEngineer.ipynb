{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features to add\n",
    "- essay \"perplexity\" and variance in perplexity.  Seems to be the most important predictor in the literature.\n",
    "\n",
    "### Additional features to add (lower priority)\n",
    "- length of writing\n",
    "- average length of sentence\n",
    "- average complexity of a word\n",
    "- percentage of text that is punctuation\n",
    "- percentage of text that is commas (or average # of commas per sentence)\n",
    "- proportion of text that are nouns/verbs/etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "import nltk.tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "asap_df = pd.read_excel(\"../cleanData/processedAsap.xlsx\")\n",
    "test_row = asap_df.iloc[3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple features ###\n",
    "\n",
    "#create feature for number of characters\n",
    "asap_df[\"character_count\"] = asap_df[\"essay\"].apply(lambda x: len(x) - x.count(\" \")) #calculate length\n",
    "\n",
    "#create feature for number of words\n",
    "def count_words(word_token_text):\n",
    "    len(word_token_text) #CHECK.  NOT  PERFORMING AS EXPECTED. \n",
    "asap_df[\"word_count\"] = asap_df[\"word_tokens\"].apply(lambda x:  count_words(x))\n",
    "\n",
    "#create feature for number of sentences\n",
    "\n",
    "\n",
    "#create feature for % of text that is punctuation\n",
    "def count_punctuation(text):\n",
    "    punct_count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(punct_count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "asap_df[\"punct_pct\"] = asap_df[\"essay\"].apply(lambda x: count_punctuation(x)) #run code to calc %punctuation feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paramdeep's code integrated into df\n",
    "\n",
    "def avg_sentence_length(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    \n",
    "    # Calculate the total number of words and sentences\n",
    "    num_words = sum(len(sentence.split()) for sentence in sentences)\n",
    "    num_sentences = len(sentences)\n",
    "    \n",
    "    # Calculate the average sentence length\n",
    "    avg_length = num_words / num_sentences\n",
    "    \n",
    "    return avg_length\n",
    "\n",
    "asap_df[\"avg_words_per_sentence\"] = asap_df[\"essay\"].apply(lambda x: avg_sentence_length(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "asap_df.to_excel(\"../cleanData/featuresAsap.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to merge into feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maisam's code\n",
    "filename = input(\"Enter the name of the file: \")\n",
    "try:\n",
    "    with open(filename, \"r\") as file:\n",
    "        contents = file.read()\n",
    "        lines = contents.split('\\n')\n",
    "        num_lines = len(lines)\n",
    "        num_words = sum(len(line.split()) for line in lines)\n",
    "        print(f\"The file '{filename}' has {num_lines} lines and {num_words} words.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{filename}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paramdeep's code\n",
    "\n",
    "#This code defines a function extract_features that takes a text as input and returns a dictionary of features that could be characteristic of text generated by AI. \n",
    "#In this example, the function checks for features related to sentence structure and syntax, distinctive vocabulary and word choice, overall coherence and flow, and use of examples and evidence.\n",
    "#Need to install nltk before we can proceed\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define a function to extract features from a given text\n",
    "def extract_features(text):\n",
    "    features = {}\n",
    "    \n",
    "    # Tokenize the text into sentences and words\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Check for sentence structure and syntax patterns\n",
    "    if any('.' in sentence for sentence in sentences):\n",
    "        features['uses_period'] = True\n",
    "    if any('!' in sentence for sentence in sentences):\n",
    "        features['uses_exclamation'] = True\n",
    "    if any('?' in sentence for sentence in sentences):\n",
    "        features['uses_question'] = True\n",
    "        \n",
    "    # Check for distinctive vocabulary and word choice\n",
    "    common_words = ['computer', 'technology', 'internet']\n",
    "    common_word_count = sum(1 for word in words if word.lower() in common_words)\n",
    "    if common_word_count > 2:\n",
    "        features['uses_common_words'] = True\n",
    "        \n",
    "    # Check for overall coherence and flow\n",
    "    if len(sentences) > 3:\n",
    "        sentence_lengths = [len(sentence) for sentence in sentences]\n",
    "        if max(sentence_lengths) > 100:\n",
    "            features['long_sentences'] = True\n",
    "    \n",
    "    # Check for use of examples and evidence\n",
    "    if any('for example' in sentence for sentence in sentences):\n",
    "        features['uses_example'] = True\n",
    "    if any('according to' in sentence for sentence in sentences):\n",
    "        features['uses_evidence'] = True\n",
    "    \n",
    "    return features\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
